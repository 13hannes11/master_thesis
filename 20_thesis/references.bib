
@article{hanna2019whole,
  title = {Whole Slide Imaging Equivalency and Efficiency Study: Experience at a Large Academic Center},
  shorttitle = {Whole Slide Imaging Equivalency and Efficiency Study},
  author = {Hanna, Matthew G. and Reuter, Victor E. and Hameed, Meera R. and Tan, Lee K. and Chiang, Sarah and Sigel, Carlie and Hollmann, Travis and Giri, Dilip and Samboy, Jennifer and Moradel, Carlos and Rosado, Andrea and Otilano, John R. and England, Christine and Corsale, Lorraine and Stamelos, Evangelos and Yagi, Yukako and Schüffler, Peter J. and Fuchs, Thomas and Klimstra, David S. and Sirintrapun, S. Joseph},
  date = {2019-07},
  journaltitle = {Modern Pathology},
  shortjournal = {Mod Pathol},
  volume = {32},
  number = {7},
  pages = {916--928},
  issn = {0893-3952, 1530-0285},
  doi = {10/gg3wtj},
  abstract = {Whole slide imaging is Food and Drug Administration-approved for primary diagnosis in the United States of America; however, relatively few pathology departments in the country have fully implemented an enterprise wide digital pathology system enabled for primary diagnosis. Digital pathology has significant potential to transform pathology practice with several published studies documenting some level of diagnostic equivalence between digital and conventional systems. However, whole slide imaging also has significant potential to disrupt pathology practice, due to the differences in efficiency of manipulating digital images vis-à-vis glass slides, and studies on the efficiency of actual digital pathology workload are lacking. Our randomized, equivalency and efficiency study aimed to replicate clinical workflow, comparing conventional microscopy to a complete digital pathology signout using whole slide images, evaluating the equivalency and efficiency of glass slide to whole slide image reporting, reflective of true pathology practice workloads in the clinical setting. All glass slides representing an entire day’s routine clinical signout workload for six different anatomic pathology subspecialties at Memorial Sloan Kettering Cancer Center were scanned on Leica Aperio AT2 at ×40 (0.25 µm/pixel). Integration of whole slide images for each accessioned case is through an interface between the Leica eSlide manager database and the laboratory information system, Cerner CoPathPlus. Pathologists utilized a standard institution computer workstation and viewed whole slide images through an internally developed, vendor agnostic whole slide image viewer, named the “MSK Slide Viewer”. Subspecialized pathologists first reported on glass slides from surgical pathology cases using routine clinical workflow. Glass slides were de-identified, scanned, and re-accessioned in the laboratory information system test environment. After a washout period of 13 weeks, pathologists reported the same clinical workload using whole slide image integrated within the laboratory information system. Intraobserver equivalency metrics included top-line diagnosis, margin status, lymphovascular and/or perineural invasion, pathology stage, and the need to order ancillary testing (i.e., recuts, immunohistochemistry). Turnaround time (efficiency) evaluation was defined by the start of each case when opened in the laboratory information system and when the case was completed for that day (i.e., case sent to signout queue or pending ancillary studies). Eight pathologists participated from the following subspecialties: bone and soft tissue, genitourinary, gastrointestinal, breast, gynecologic, and dermatopathology. Glass slides signouts comprised of 204 cases, encompassing 2091 glass slides; and digital signouts comprised of 199 cases, encompassing 2073 whole slide images. The median whole slide image file size was 1.54 GB; scan time/slide, 6 min 24 s; and scan area 32.1 × 18.52 mm. Overall diagnostic equivalency (e.g., top-line diagnosis) was 99.3\% between digital and glass slide signout; however, signout using whole slide images showed a median overall 19\% decrease in efficiency per case. No significant difference by reader, subspecialty, or specimen type was identified. Our experience is the most comprehensive study to date and shows high intraobserver whole slide image to glass slide equivalence in reporting of true clinical workflows and workloads. Efficiency needs to improve for digital pathology to gain more traction among pathologists.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/KFI5AQNG/Hanna et al. - 2019 - Whole slide imaging equivalency and efficiency stu.pdf}
}

@article{hosseini2020focus,
  title = {Focus {{Quality Assessment}} of {{High-Throughput Whole Slide Imaging}} in {{Digital Pathology}}},
  author = {Hosseini, Mahdi S. and Brawley-Hayes, Jasper A. Z. and Zhang, Yueyang and Chan, Lyndon and Plataniotis, Konstantinos and Damaskinos, Savvas},
  date = {2020-01},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {39},
  number = {1},
  pages = {62--74},
  issn = {0278-0062, 1558-254X},
  doi = {10/gm8ghb},
  abstract = {One of the challenges facing the adoption of digital pathology workflows for clinical use is the need for automated quality control. As the scanners sometimes determine focus inaccurately, the resultant image blur deteriorates the scanned slide to the point of being unusable. Also, the scanned slide images tend to be extremely large when scanned at greater or equal 20X image resolution. Hence, for digital pathology to be clinically useful, it is necessary to use computational tools to quickly and accurately quantify the image focus quality and determine whether an image needs to be re-scanned. We propose a no-reference focus quality assessment metric specifically for digital pathology images that operate by using a sum of even-derivative filter bases to synthesize a human visual system-like kernel, which is modeled as the inverse of the lens’ point spread function. This kernel is then applied to a digital pathology image to modify high-frequency image information deteriorated by the scanner’s optics and quantify the focus quality at the patch level. We show in several experiments that our method correlates better with ground-truth z-level data than other methods, which is more computationally efficient. We also extend our method to generate a local slide-level focus quality heatmap, which can be used for automated slide quality control, and demonstrate the utility of our method for clinical scan quality control by comparison with subjective slide quality scores.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/6X5KDYCX/Hosseini et al. - 2020 - Focus Quality Assessment of High-Throughput Whole .pdf}
}

@article{kohlberger2019wholeslide,
  title = {Whole-{{Slide Image Focus Quality}}: {{Automatic Assessment}} and {{Impact}} on {{AI Cancer Detection}}},
  shorttitle = {Whole-{{Slide Image Focus Quality}}},
  author = {Kohlberger, Timo and Liu, Yun and Moran, Melissa and Po-Hsuan and Chen and Brown, Trissia and Mermel, Craig H. and Hipp, Jason D. and Stumpe, Martin C.},
  date = {2019},
  journaltitle = {Journal of Pathology Informatics},
  shortjournal = {J Pathol Inform},
  volume = {10},
  number = {1},
  eprint = {1901.04619},
  eprinttype = {arxiv},
  pages = {39},
  issn = {2153-3539},
  doi = {10/gm9n9g},
  abstract = {Digital pathology enables remote access or consults and powerful image analysis algorithms. However, the slide digitization process can create artifacts such as out-of-focus (OOF). OOF is often only detected upon careful review, potentially causing rescanning and workflow delays. Although scan-time operator screening for whole-slide OOF is feasible, manual screening for OOF affecting only parts of a slide is impractical. We developed a convolutional neural network (ConvFocus) to exhaustively localize and quantify the severity of OOF regions on digitized slides. ConvFocus was developed using our refined semi-synthetic OOF data generation process, and evaluated using real whole-slide images spanning 3 different tissue types and 3 different stain types that were digitized by two different scanners. ConvFocus's predictions were compared with pathologist-annotated focus quality grades across 514 distinct regions representing 37,700 35x35 \$\textbackslash mu\$m image patches, and 21 digitized "z-stack" whole-slide images that contain known OOF patterns. When compared to pathologist-graded focus quality, ConvFocus achieved Spearman rank coefficients of 0.81 and 0.94 on two scanners, and reproduced the expected OOF patterns from z-stack scanning. We also evaluated the impact of OOF on the accuracy of a state-of-the-art metastatic breast cancer detector and saw a consistent decrease in performance with increasing OOF. Comprehensive whole-slide OOF categorization could enable rescans prior to pathologist review, potentially reducing the impact of digitization focus issues on the clinical workflow. We show that the algorithm trained on our semi-synthetic OOF data generalizes well to real OOF regions across tissue types, stains, and scanners. Finally, quantitative OOF maps can flag regions that might otherwise be misclassified by image analysis algorithms, preventing OOF-induced errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,foucs detection},
  file = {/var/home/hannes/Zotero/storage/AY98CIUF/Kohlberger et al. - 2019 - Whole-Slide Image Focus Quality Automatic Assessm.pdf;/var/home/hannes/Zotero/storage/QEUQUHCG/1901.html}
}

@article{osibote2010automated,
  title = {Automated Focusing in Bright-Field Microscopy for Tuberculosis Detection: {{BRIGHT-FIELD AUTOFOCUSING FOR TUBERCULOSIS DETECTION}}},
  shorttitle = {Automated Focusing in Bright-Field Microscopy for Tuberculosis Detection},
  author = {Osibote, O.A. and Dendere, R. and Krishnan, S. and Douglas, T.S.},
  date = {2010-11},
  journaltitle = {Journal of Microscopy},
  volume = {240},
  number = {2},
  pages = {155--163},
  issn = {00222720},
  doi = {10/fcftsf},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/CGEQ4856/Osibote et al. - 2010 - Automated focusing in bright-field microscopy for .pdf}
}

@article{senaras2018deepfocus,
  title = {{{DeepFocus}}: {{Detection}} of out-of-Focus Regions in Whole Slide Digital Images Using Deep Learning},
  shorttitle = {{{DeepFocus}}},
  author = {Senaras, Caglar and Niazi, M. Khalid Khan and Lozanski, Gerard and Gurcan, Metin N.},
  editor = {Lo, Chung-Ming},
  date = {2018-10-25},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {10},
  pages = {e0205387},
  issn = {1932-6203},
  doi = {10/gm9n8p},
  langid = {english},
  keywords = {foucs detection},
  annotation = {38 citations (Crossref) [2022-01-17]},
  file = {/var/home/hannes/Zotero/storage/M9VZ57ZF/Senaras et al. - 2018 - DeepFocus Detection of out-of-focus regions in wh.pdf}
}

@inproceedings{wang2020focuslitenn,
  title = {{{FocusLiteNN}}: {{High Efficiency Focus Quality Assessment}} for {{Digital Pathology}}},
  shorttitle = {{{FocusLiteNN}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2020},
  author = {Wang, Zhongling and Hosseini, Mahdi S. and Miles, Adyn and Plataniotis, Konstantinos N. and Wang, Zhou},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {403--413},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10/gnpd37},
  abstract = {Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Deep learning,Digital pathology,Focus Quality Assessment,Out-of-focus,Whole Slide Image (WSI)},
  file = {/var/home/hannes/Zotero/storage/X28NQIFF/Wang et al. - 2020 - FocusLiteNN High Efficiency Focus Quality Assessm.pdf}
}

@article{xiang2021autofocus,
  title = {Autofocus of Whole Slide Imaging Based on Convolution and Recurrent Neural Networks},
  author = {Xiang, Yao and He, Zhujun and Liu, Qing and Chen, Jialin and Liang, Yixiong},
  date = {2021-01},
  journaltitle = {Ultramicroscopy},
  shortjournal = {Ultramicroscopy},
  volume = {220},
  pages = {113146},
  issn = {03043991},
  doi = {10/gnpqnz},
  abstract = {During the process of whole slide imaging, it is necessary to focus thousands of fields of view to obtain a high-quality image. To make the focusing procedure efficient and effective, we propose a novel autofocus algorithm for whole slide imaging. It is based on convolution and recurrent neural networks to predict the out-of-focus distance and subsequently update the focus location of the camera lens in an iterative manner. More specifically, we train a convolution neural network to extract focus information in the form of a focus feature vector. In order to make the prediction more accurate, we apply a recurrent neural network to combine focus information from previous search iteration and current search iteration to form a feature aggregation vector. This vector contains more focus information than the previous one and is subsequently used to predict the out-of-focus distance. Our experiments indicate that our proposed autofocus algorithm is able to rapidly determine the optimal in-focus image. The code is available at https://github.com/hezhujun/autofocus-rnn.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/PXVKGKHJ/Xiang et al. - 2021 - Autofocus of whole slide imaging based on convolut.pdf}
}


