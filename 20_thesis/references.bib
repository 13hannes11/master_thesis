
@article{feasey2010neglected,
  title = {Neglected Tropical Diseases},
  author = {Feasey, N. and Wansbrough-Jones, M. and Mabey, D. C. W. and Solomon, A. W.},
  date = {2010-03-01},
  journaltitle = {British Medical Bulletin},
  shortjournal = {British Medical Bulletin},
  volume = {93},
  number = {1},
  pages = {179--200},
  issn = {0007-1420, 1471-8391},
  doi = {10/fm3j5b},
  abstract = {Introduction: The neglected tropical diseases (NTDs) are infectious diseases that principally impact the world’s poorest people. They have been neglected for decades, initially as part of a general disregard for the developing world, and more recently due to the intensity of focus on HIV/AIDS, tuberculosis and malaria. Sources of data: Primary research and review articles were selected for inclusion using searches of PubMed and our existing collections. Results: There have been recent notable successes in NTD control. Dracunculiasis is approaching eradication. Leprosy and onchocerciasis are in decline. There are ambitious plans to eliminate trachoma and lymphatic filariasis. Investment in NTD control has high rates of economic return. Conclusion: Although there are proven strategies to control several NTDs, these diseases continue to cause a massive burden of morbidity. There is urgent need for more basic and operational research, drug and vaccine development, and greater prioritization by governments and international agencies.},
  langid = {english},
  keywords = {Neglected Tropical Diseases},
  file = {/var/home/hannes/Zotero/storage/QZEMMS7X/Feasey et al. - 2010 - Neglected tropical diseases.pdf}
}

@article{hanna2019whole,
  title = {Whole Slide Imaging Equivalency and Efficiency Study: Experience at a Large Academic Center},
  shorttitle = {Whole Slide Imaging Equivalency and Efficiency Study},
  author = {Hanna, Matthew G. and Reuter, Victor E. and Hameed, Meera R. and Tan, Lee K. and Chiang, Sarah and Sigel, Carlie and Hollmann, Travis and Giri, Dilip and Samboy, Jennifer and Moradel, Carlos and Rosado, Andrea and Otilano, John R. and England, Christine and Corsale, Lorraine and Stamelos, Evangelos and Yagi, Yukako and Schüffler, Peter J. and Fuchs, Thomas and Klimstra, David S. and Sirintrapun, S. Joseph},
  date = {2019-07},
  journaltitle = {Modern Pathology},
  shortjournal = {Mod Pathol},
  volume = {32},
  number = {7},
  pages = {916--928},
  issn = {0893-3952, 1530-0285},
  doi = {10/gg3wtj},
  abstract = {Whole slide imaging is Food and Drug Administration-approved for primary diagnosis in the United States of America; however, relatively few pathology departments in the country have fully implemented an enterprise wide digital pathology system enabled for primary diagnosis. Digital pathology has significant potential to transform pathology practice with several published studies documenting some level of diagnostic equivalence between digital and conventional systems. However, whole slide imaging also has significant potential to disrupt pathology practice, due to the differences in efficiency of manipulating digital images vis-à-vis glass slides, and studies on the efficiency of actual digital pathology workload are lacking. Our randomized, equivalency and efficiency study aimed to replicate clinical workflow, comparing conventional microscopy to a complete digital pathology signout using whole slide images, evaluating the equivalency and efficiency of glass slide to whole slide image reporting, reflective of true pathology practice workloads in the clinical setting. All glass slides representing an entire day’s routine clinical signout workload for six different anatomic pathology subspecialties at Memorial Sloan Kettering Cancer Center were scanned on Leica Aperio AT2 at ×40 (0.25 µm/pixel). Integration of whole slide images for each accessioned case is through an interface between the Leica eSlide manager database and the laboratory information system, Cerner CoPathPlus. Pathologists utilized a standard institution computer workstation and viewed whole slide images through an internally developed, vendor agnostic whole slide image viewer, named the “MSK Slide Viewer”. Subspecialized pathologists first reported on glass slides from surgical pathology cases using routine clinical workflow. Glass slides were de-identified, scanned, and re-accessioned in the laboratory information system test environment. After a washout period of 13 weeks, pathologists reported the same clinical workload using whole slide image integrated within the laboratory information system. Intraobserver equivalency metrics included top-line diagnosis, margin status, lymphovascular and/or perineural invasion, pathology stage, and the need to order ancillary testing (i.e., recuts, immunohistochemistry). Turnaround time (efficiency) evaluation was defined by the start of each case when opened in the laboratory information system and when the case was completed for that day (i.e., case sent to signout queue or pending ancillary studies). Eight pathologists participated from the following subspecialties: bone and soft tissue, genitourinary, gastrointestinal, breast, gynecologic, and dermatopathology. Glass slides signouts comprised of 204 cases, encompassing 2091 glass slides; and digital signouts comprised of 199 cases, encompassing 2073 whole slide images. The median whole slide image file size was 1.54 GB; scan time/slide, 6 min 24 s; and scan area 32.1 × 18.52 mm. Overall diagnostic equivalency (e.g., top-line diagnosis) was 99.3\% between digital and glass slide signout; however, signout using whole slide images showed a median overall 19\% decrease in efficiency per case. No significant difference by reader, subspecialty, or specimen type was identified. Our experience is the most comprehensive study to date and shows high intraobserver whole slide image to glass slide equivalence in reporting of true clinical workflows and workloads. Efficiency needs to improve for digital pathology to gain more traction among pathologists.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/KFI5AQNG/Hanna et al. - 2019 - Whole slide imaging equivalency and efficiency stu.pdf}
}

@article{hosseini2020focus,
  title = {Focus {{Quality Assessment}} of {{High-Throughput Whole Slide Imaging}} in {{Digital Pathology}}},
  author = {Hosseini, Mahdi S. and Brawley-Hayes, Jasper A. Z. and Zhang, Yueyang and Chan, Lyndon and Plataniotis, Konstantinos and Damaskinos, Savvas},
  date = {2020-01},
  journaltitle = {IEEE Transactions on Medical Imaging},
  shortjournal = {IEEE Trans. Med. Imaging},
  volume = {39},
  number = {1},
  pages = {62--74},
  issn = {0278-0062, 1558-254X},
  doi = {10/gm8ghb},
  abstract = {One of the challenges facing the adoption of digital pathology workflows for clinical use is the need for automated quality control. As the scanners sometimes determine focus inaccurately, the resultant image blur deteriorates the scanned slide to the point of being unusable. Also, the scanned slide images tend to be extremely large when scanned at greater or equal 20X image resolution. Hence, for digital pathology to be clinically useful, it is necessary to use computational tools to quickly and accurately quantify the image focus quality and determine whether an image needs to be re-scanned. We propose a no-reference focus quality assessment metric specifically for digital pathology images that operate by using a sum of even-derivative filter bases to synthesize a human visual system-like kernel, which is modeled as the inverse of the lens’ point spread function. This kernel is then applied to a digital pathology image to modify high-frequency image information deteriorated by the scanner’s optics and quantify the focus quality at the patch level. We show in several experiments that our method correlates better with ground-truth z-level data than other methods, which is more computationally efficient. We also extend our method to generate a local slide-level focus quality heatmap, which can be used for automated slide quality control, and demonstrate the utility of our method for clinical scan quality control by comparison with subjective slide quality scores.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/6X5KDYCX/Hosseini et al. - 2020 - Focus Quality Assessment of High-Throughput Whole .pdf}
}

@incollection{jamison2006chapter,
  title = {Chapter 24. {{Helminth Infections}}: {{Soil-Transmitted Helminth Infections}} and {{Schistosomiasis}}},
  shorttitle = {Chapter 24. {{Helminth Infections}}},
  booktitle = {Disease {{Control Priorities}} in {{Developing Countries}} (2nd {{Edition}})},
  editor = {Jamison, Dean T. and Breman, Joel G. and Measham, Anthony R. and Alleyne, George and Claeson, Mariam and Evans, David B. and Jha, Prabhat and Mills, Anne and Musgrove, Philip},
  date = {2006-04-02},
  pages = {467--482},
  publisher = {{World Bank Publications}},
  doi = {10.1596/978-0-8213-6179-5/Chpt-24},
  isbn = {978-0-8213-6179-5},
  langid = {english},
  keywords = {ascaris,hookworm,Neglected Tropical Diseases,schistosoma,Soil-Transmitted Helminths,whipworm},
  file = {/var/home/hannes/Zotero/storage/8VE6JXLH/Jamison et al. - 2006 - Chapter 24. Helminth Infections Soil-Transmitted .pdf}
}

@article{jourdan2018soiltransmitted,
  title = {Soil-Transmitted Helminth Infections},
  author = {Jourdan, Peter Mark and Lamberton, Poppy H L and Fenwick, Alan and Addiss, David G},
  date = {2018-01},
  journaltitle = {The Lancet},
  shortjournal = {The Lancet},
  volume = {391},
  number = {10117},
  pages = {252--265},
  issn = {01406736},
  doi = {10/gczb9g},
  langid = {english},
  keywords = {ascaris,hookworm,Neglected Tropical Diseases,Soil-Transmitted Helminths,whipworm},
  file = {/var/home/hannes/Zotero/storage/7ZN2MNLC/Jourdan et al. - 2018 - Soil-transmitted helminth infections.pdf}
}

@article{kohlberger2019wholeslide,
  title = {Whole-{{Slide Image Focus Quality}}: {{Automatic Assessment}} and {{Impact}} on {{AI Cancer Detection}}},
  shorttitle = {Whole-{{Slide Image Focus Quality}}},
  author = {Kohlberger, Timo and Liu, Yun and Moran, Melissa and Po-Hsuan and Chen and Brown, Trissia and Mermel, Craig H. and Hipp, Jason D. and Stumpe, Martin C.},
  date = {2019},
  journaltitle = {Journal of Pathology Informatics},
  shortjournal = {J Pathol Inform},
  volume = {10},
  number = {1},
  eprint = {1901.04619},
  eprinttype = {arxiv},
  pages = {39},
  issn = {2153-3539},
  doi = {10/gm9n9g},
  abstract = {Digital pathology enables remote access or consults and powerful image analysis algorithms. However, the slide digitization process can create artifacts such as out-of-focus (OOF). OOF is often only detected upon careful review, potentially causing rescanning and workflow delays. Although scan-time operator screening for whole-slide OOF is feasible, manual screening for OOF affecting only parts of a slide is impractical. We developed a convolutional neural network (ConvFocus) to exhaustively localize and quantify the severity of OOF regions on digitized slides. ConvFocus was developed using our refined semi-synthetic OOF data generation process, and evaluated using real whole-slide images spanning 3 different tissue types and 3 different stain types that were digitized by two different scanners. ConvFocus's predictions were compared with pathologist-annotated focus quality grades across 514 distinct regions representing 37,700 35x35 \$\textbackslash mu\$m image patches, and 21 digitized "z-stack" whole-slide images that contain known OOF patterns. When compared to pathologist-graded focus quality, ConvFocus achieved Spearman rank coefficients of 0.81 and 0.94 on two scanners, and reproduced the expected OOF patterns from z-stack scanning. We also evaluated the impact of OOF on the accuracy of a state-of-the-art metastatic breast cancer detector and saw a consistent decrease in performance with increasing OOF. Comprehensive whole-slide OOF categorization could enable rescans prior to pathologist review, potentially reducing the impact of digitization focus issues on the clinical workflow. We show that the algorithm trained on our semi-synthetic OOF data generalizes well to real OOF regions across tissue types, stains, and scanners. Finally, quantitative OOF maps can flag regions that might otherwise be misclassified by image analysis algorithms, preventing OOF-induced errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,foucs detection},
  file = {/var/home/hannes/Zotero/storage/AY98CIUF/Kohlberger et al. - 2019 - Whole-Slide Image Focus Quality Automatic Assessm.pdf;/var/home/hannes/Zotero/storage/QEUQUHCG/1901.html}
}

@thesis{larsson2020developmenta,
  type = {Examensarbete},
  title = {Development of Machine Learning Models for Object Identification of Parasite Eggs Using Microscopy},
  author = {Larsson, Joel and Hedberg, Rasmus},
  date = {2020},
  institution = {{Uppsala University}},
  location = {{Uppsala}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-414386},
  urldate = {2022-01-26},
  abstract = {Over one billion people in developing countries are afflicted by parasitic infections caused by soil-transmitted helminths. These infections are treatable with cheap and safe medicine that is widely available. However, diagnosis of these infections has proven to be a bottleneck by the fact that it is time-consuming, requires expensive equipment and trained personnel to be consistent and accurate. This study aimed to investigate the viability and performance of five machine learning models and a 'modular neural network' approach to localize and classify the following parasite eggs in microscopic images: Ascaris lumbricoides, Trichuris trichuria, Hookworm and Schistosoma mansoni. These models were implemented and evaluated on the Nvidia Jetson AGX Xavier to establish that they fulfilled the specifications of 95\textbackslash\% specificity and sensitivity, but also a speed requirement of 40000 images per 24 hours. The results show that R-FCN ResNet101 was the best model produced in this study, which performed the best on average. However, it did not fulfill the specifications entirely but is still considered a success due to being an improvement to the current implementation at Etteplan. Evaluation of the modular neural network approach would require further investigation to verify the performance of the system, but the results indicate it could be a possible improvement to the off-the-shelf machine learning models. To conclude, the study showed that the data and data infrastructure provided by Etteplan has proven to be a very powerful tool in training machine learning models to classify and localize parasite eggs in stool samples. However, expansion of the data to reduce the imbalance between the representations of the classes but also include more patient information could improve the training and evaluation process of the models.},
  langid = {english},
  pagetotal = {34},
  file = {/var/home/hannes/Zotero/storage/X4VAW9B4/Larsson and Hedberg - 2020 - Development of machine learning models for object .pdf}
}

@article{oliveira2000haemozoin,
  title = {Haemozoin in {{Schistosoma}} Mansoni},
  author = {Oliveira, Marcus F and d' Avila, Joana C.P and Torres, Christiane R and Oliveira, Pedro L and Tempone, Antônio J and Rumjanek, Franklin D and Braga, Cláudia M.S and Silva, José R and Dansa-Petretski, Marı́lvia and Oliveira, Marco A and de Souza, Wanderley and Ferreira, Sérgio T},
  options = {useprefix=true},
  date = {2000-11},
  journaltitle = {Molecular and Biochemical Parasitology},
  shortjournal = {Molecular and Biochemical Parasitology},
  volume = {111},
  number = {1},
  pages = {217--221},
  issn = {01666851},
  doi = {10/dz9sz4},
  langid = {english},
  keywords = {Neglected Tropical Diseases,schistosoma},
  file = {/var/home/hannes/Zotero/storage/4ZCCEREK/Oliveira et al. - 2000 - Haemozoin in Schistosoma mansoni.pdf}
}

@article{osibote2010automated,
  title = {Automated Focusing in Bright-Field Microscopy for Tuberculosis Detection: {{BRIGHT-FIELD AUTOFOCUSING FOR TUBERCULOSIS DETECTION}}},
  shorttitle = {Automated Focusing in Bright-Field Microscopy for Tuberculosis Detection},
  author = {Osibote, O.A. and Dendere, R. and Krishnan, S. and Douglas, T.S.},
  date = {2010-11},
  journaltitle = {Journal of Microscopy},
  volume = {240},
  number = {2},
  pages = {155--163},
  issn = {00222720},
  doi = {10/fcftsf},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/CGEQ4856/Osibote et al. - 2010 - Automated focusing in bright-field microscopy for .pdf}
}

@article{senaras2018deepfocus,
  title = {{{DeepFocus}}: {{Detection}} of out-of-Focus Regions in Whole Slide Digital Images Using Deep Learning},
  shorttitle = {{{DeepFocus}}},
  author = {Senaras, Caglar and Niazi, M. Khalid Khan and Lozanski, Gerard and Gurcan, Metin N.},
  editor = {Lo, Chung-Ming},
  date = {2018-10-25},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {10},
  pages = {e0205387},
  issn = {1932-6203},
  doi = {10/gm9n8p},
  langid = {english},
  keywords = {foucs detection},
  annotation = {38 citations (Crossref) [2022-01-17]},
  file = {/var/home/hannes/Zotero/storage/M9VZ57ZF/Senaras et al. - 2018 - DeepFocus Detection of out-of-focus regions in wh.pdf}
}

@inproceedings{wang2020focuslitenn,
  title = {{{FocusLiteNN}}: {{High Efficiency Focus Quality Assessment}} for {{Digital Pathology}}},
  shorttitle = {{{FocusLiteNN}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} – {{MICCAI}} 2020},
  author = {Wang, Zhongling and Hosseini, Mahdi S. and Miles, Adyn and Plataniotis, Konstantinos N. and Wang, Zhou},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {403--413},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10/gnpd37},
  abstract = {Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Deep learning,Digital pathology,Focus Quality Assessment,Out-of-focus,Whole Slide Image (WSI)},
  file = {/var/home/hannes/Zotero/storage/X28NQIFF/Wang et al. - 2020 - FocusLiteNN High Efficiency Focus Quality Assessm.pdf}
}

@article{xiang2021autofocus,
  title = {Autofocus of Whole Slide Imaging Based on Convolution and Recurrent Neural Networks},
  author = {Xiang, Yao and He, Zhujun and Liu, Qing and Chen, Jialin and Liang, Yixiong},
  date = {2021-01},
  journaltitle = {Ultramicroscopy},
  shortjournal = {Ultramicroscopy},
  volume = {220},
  pages = {113146},
  issn = {03043991},
  doi = {10/gnpqnz},
  abstract = {During the process of whole slide imaging, it is necessary to focus thousands of fields of view to obtain a high-quality image. To make the focusing procedure efficient and effective, we propose a novel autofocus algorithm for whole slide imaging. It is based on convolution and recurrent neural networks to predict the out-of-focus distance and subsequently update the focus location of the camera lens in an iterative manner. More specifically, we train a convolution neural network to extract focus information in the form of a focus feature vector. In order to make the prediction more accurate, we apply a recurrent neural network to combine focus information from previous search iteration and current search iteration to form a feature aggregation vector. This vector contains more focus information than the previous one and is subsequently used to predict the out-of-focus distance. Our experiments indicate that our proposed autofocus algorithm is able to rapidly determine the optimal in-focus image. The code is available at https://github.com/hezhujun/autofocus-rnn.},
  langid = {english},
  file = {/var/home/hannes/Zotero/storage/PXVKGKHJ/Xiang et al. - 2021 - Autofocus of whole slide imaging based on convolut.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

