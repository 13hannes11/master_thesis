\chapter{Foundations}
\label{ch:Foundations}

This chapter introduces relevant background information about \acfp{ntd}, which method, namely Kato-Katz \cite{katz1972simple}, is used to detect eggs for diagnosis. Further, it elaborates on \acf{wsi}, an important topic for digital pathology. Moreover, \acfp{nn} are introduced, and it is shown how they work. Further developments and architecture is presented. Lastly, related studies are introduced.

\section{Neglected Tropical Diseases}
\label{sec:Foundations:NTDs}

\Acfp{ntd} \cite{feasey2010neglected} are a biologically diverse group of diseases which mainly occur in tropical regions. The geographic location, however, is not linked to temperatures or climate but mainly as a result of a large part of the world's poorest population living in these regions. \Acp{ntd} affect approximately a sixth of the world population, yet, they have received less attention than other diseases. The economic effects for infected patience are detrimental as they can exceed the annual household earnings by more than twofold \cite{conteh2010socioeconomic}. Further, infections of children are related to reduced educational performance and have negative effects on future income prospects. \textcite{feasey2010neglected} state that there are \acp{ntd} which have a high prevalence and excellent potential for successful control. Among these are Ascaris, hookworm, whipworm, and Schistosoma. These parasites are the ones present in the dataset used in this thesis. Ascaris, whipworm and hookworm are \acfp{sth} which are considered to be \acp{ntd} \cite{mbongngwese2020diagnostic}. 

\subsection{Soil-Transmitted Helminth} % Soil-Transmitted Helminths
\label{sec:Foundations:NTDs:STHs} 

Helminthic parasites that are mainly transmitted through soil are known as \acfp{sth} \cite{feasey2010neglected,jourdan2018soiltransmitted}. This section gives a more detailed introduction into \acp{sth} present in the dataset used in this thesis.


\subsubsection{Roundworm (Ascaris)}
\label{sec:Foundations:NTDs:STHs:Ascaris}

The roundworm (Ascaris) is the most common among \acp{sth} \cite{jamison2006helminth}. Upon oral ingestion ascaris eggs hatch and larvae move through the lungs to settle into the small intestine where they grow into adult worms \cite{jourdan2018soiltransmitted}. There they reproduce sexually and produce eggs which are expelled through human faeces. Eggs in warm, moist soil can infect other humans for years. An illustration of a roundworm can be seen in \autoref{fig:Foundations:NCLs:STHs:Ascaris}.

\subsubsection{Whipworm (Trichuris)}
\label{sec:Foundations:NTDs:STHs:Whipworm}

Like roundworms whipworms are also transmitted through the faecal-oral cycle, where eggs are ingested via food or hands \cite{jourdan2018soiltransmitted}. Whipworms also reproduce in the small intestine, however, unlike roundworms, they do not migrate through the lungs. \autoref{fig:Foundations:NCLs:STHs:Whipworm:Adult} shows an illustration of a whipworm.

\subsubsection{Hookworm (Necator Americanus and Ancylostoma Duodenale)}
\label{sec:Foundations:NTDs:STHs:Hookworm}

Unlike roundworms and whipworms, hookworms' eggs hatch after around 5-10 days outside the human body. Roundworm larva then infect humans by penetrating the skin of the feet \cite{jourdan2018soiltransmitted}. They then travel through the lungs into the voice box (larynx) where they are swallowed. Finally, hookworms, like roundworms and whipworms settle in the small intestine and produce eggs which leave the human body through faeces. An image of a hookworm larva can be seen in \autoref{fig:Foundations:NCLs:STHs:Hookworm:Adult}.

\begin{figure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Ascaris_adult_enlarged.jpg}
        \caption{Illustration of a \textbf{roundworm} by \textcite{blainville1824traite}.}
        \label{fig:Foundations:NCLs:STHs:Ascaris}
        \vspace*{2mm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Trichuris_trichiura_adult_female_enlarged.jpg}
        \caption{Illustration of a female \textbf{whipworm} by \textcite{blainville1824traite}.}
        \label{fig:Foundations:NCLs:STHs:Whipworm:Adult}
        \vspace*{2mm}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Hookworm_larva.jpg}
        \caption{Microscopy image of a \textbf{Hookworm} larva \cite{dpdx2019hookworm}.}
        \label{fig:Foundations:NCLs:STHs:Hookworm:Adult}
    \end{subfigure}    
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Schistosoma_adult_small.jpg}
        \caption{Electron micrograph of a male \textbf{Schistosoma} by \textcite{davidwilliams2009schistosoma}.}
        \label{fig:Foundations:NCLs:Schistosoma:Adult}
    \end{subfigure}
    \caption{Overview of parasitic worms. The worms shown are: Roundworm, whipworm, hookworm, and Schistosoma (left to right, top to bottom).}
    \label{fig:Foundations:NCLs:Overview}
\end{figure}


\subsection{Schistosoma}
\label{sec:Foundations:NTDs:STHs:Schistosoma}

Schistosoma worms have the most complex life cycle among parasite covered in this thesis. The excrement of an infected human contains schistosomes' eggs \cite{nelwan2019schistosomiasis}. When eggs come into contact with water they hatch and infect snails. Infected snails shed larvae (cercariae) which enter the human body through skin and migrate throughout the body using the blood stream. The location where adult worms settle depends on the exact species, but some examples are the (large and small) intestine, and the bladder. A picture of a Schistosoma worm can be seen in \autoref{fig:Foundations:NCLs:Schistosoma:Adult}.

\subsection{Diagnosis}
\label{sec:Foundations:NTDs:Diagnosis}


\subsubsection{Slide Preparation with the Kato-Katz Method}
\label{sec:Foundations:NTDs:Diagnosis:Preparation}

Schistosoma and \acp{sth} are commonly diagnosed by counting eggs present in a stool sample with a microscope. The technique used is called Kato-Katz \cite{nelwan2019schistosomiasis} and was introduced by \textcite{katz1972simple} who refined the method introduced by \citeauthor{kato1954comparative} \cite{kato1954comparative,kato1960correct}. The technique involves a cardboard sheet with a hole in it, some stainless-steel mesh-cloth, a cellophane membrane and glycerol. A sample first is pressed through the stainless steel mesh-cloth and then smeared through the hole onto the slide. Then the side is covered with a cellophane membrane and treated with glycerol \cite{mbongngwese2020diagnostic}. This technique gains fairly consistent samples, is fairly reliable, easy to perform and low cost \cite{katz1972simple}. Therefore, according to \textcite{mbongngwese2020diagnostic}, it is by the \ac{who} considered to be the \say{gold standard}.

\subsubsection{Examination}
\label{sec:Foundations:NTDs:Diagnosis:Examination}

To count the number of eggs per milligram, the slides, prepared using the Kato-Katz method, are, after a resting period, examined under a light microscope. This allows to identify the intensity of an infection of a patient \cite{feasey2010neglected}.

Instead of being examined directly by personal under a microscope, \emph{\acf{wsi}} digitizes the slides beforehand and the inspection is done on a computer. \Ac{wsi} \cite{ghaznavi2013digital, hanna2019whole, el-gabry2014wholeslide} is a process in which slides are wholistically scanned to create a digital copy. This allows diagnostic processes to be more flexible as the collection of slides is independent of its analysis. Further, digitally archived slides do not suffer any degradation. 

A commonly used focus method for \ac{wsi} is \emph{z-stacking} \cite{el-gabry2014wholeslide} which is required for slides with larger variation in topology or which have a three-dimensional structure, e.g., thick smears. A z-stack consists of multiple images taken at different focus planes. This allows a researcher to emulate the focusing behaviour of a conventional light microscope and can compensate for imperfect focusing.


\subsubsection{Egg Identification}
\label{sec:Foundations:NTDs:Diagnosis:Identification}

Eggs found under a microscope can be assigned to the corresponding parasite worm based on size, shape, and colour. \emph{Roundworm (Ascaris)} eggs are identified by a rounded thick shell with an external mamillated layer which is often stained with brown spots and has a length of \unit{45}{\micro\meter} to \unit{75}{\micro\meter} \cite{dpdx2019ascariasis} (as shown in \autoref{fig:Foundations:NCLs:Diagnosis:Ascaris:Egg}).
\emph{Whipworm} eggs (see \autoref{fig:Foundations:NCLs:Diagnosis:Whipworm:Egg}) can be identified by their elongated oval shape with two circular protrusions at the end of the longer sides \cite{dpdx2017trichuriasis,larsson2020development}. A length of approximately \unit{50}{\micro\meter} and a width of \unit{20}{\micro\meter} characterizes the eggs.
\emph{Hookworm} eggs are thin-shelled and colourless with a blurry inner structure. The eggs' size measures \unit{65}{\micro\meter} by \unit{35}{\micro\meter} which is shown in \autoref{fig:Foundations:NCLs:Diagnosis:Hookworm:Egg} \cite{dpdx2019hookworm, larsson2020development}.
Lastly, the dataset contains \emph{Schistosoma} eggs (see \autoref{fig:Foundations:NCLs:Diagnosis:Schistosoma:Egg}). They are characterized by a spine (seen as a spike) 
and have a size of approximately \unit{100}{\micro\meter} by \unit{50}{\micro\meter} which can vary by species \cite{dpdx2019schistosomiasis, larsson2020development}.

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Ascaris_egg_fertilised.jpg}
        \caption{Fertilized \textbf{roundworm} egg \cite{dpdx2019ascariasis}.}
        \label{fig:Foundations:NCLs:Diagnosis:Ascaris:Egg}
        \vspace*{2mm}
    \end{subfigure}
    \hspace*{1em}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Trichuris_trichiura_egg.jpg}
        \caption{\textbf{Whipworm} egg \cite{dpdx2017trichuriasis}.}
        \label{fig:Foundations:NCLs:Diagnosis:Whipworm:Egg}
        \vspace*{2mm}
    \end{subfigure}

    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Hookworm_egg.jpg}
        \caption{\textbf{Hookworm} egg \cite{dpdx2019hookworm}.}
        \label{fig:Foundations:NCLs:Diagnosis:Hookworm:Egg}
    \end{subfigure}
    \hspace*{1em}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/20_foundations/10_neglected_tropical/Schistosoma_egg.jpg}
        \caption{\textbf{Schistosoma} mansoni egg \cite{dpdx2019schistosomiasis}.}
        \label{fig:Foundations:NCLs:Diagnosis:Schistosoma:Egg}
    \end{subfigure}
    \caption{Overview showing the eggs of different parasites.}
    \label{fig:Foundations:NCLs:Diagnosis:Overview}
\end{figure}


\section{Artificial Neural Networks}
\label{sec:Foundations:NeuralNetworks}

\Acfp{nn}, are commonly used in machine learning applications. This section introduces how they work starting with their building blocks. Further, different types of layers are introduced, and their composition is shown. Finally, two model architecture examples give an overview over networks used in practice.

\todo[inline]{Define what a model is.}


\subsection{Feedforward Neural Networks}
\label{sec:Foundations:NeuralNetworks:FNN}

According to \textcite{nielsen2015neural} artificial neural networks were developed by \textcite{rosenblatt1958perceptron}. In feedforward neural networks information flows only from input towards the output. There is no backwards flow of information, hence, the name.
A \acf{nn} consists of multiple layers which themselves consist of individual neurons.

\subsubsection{Perceptrons and Neurons}
\label{sec:Foundations:NeuralNetworks:Perceptrons}

A perceptron (see \autoref{fig:Foundations:ANNs:Perceptron}) is a type of artificial neuron that consists of inputs ($x_1, \cdots, x_n$), weights ($w_1, \cdots, w_n$) that correspond to each input, a threshold/bias $b$ and an output \cite{nielsen2015neural}.


\tikzset{basic/.style={draw,fill=white,
                       text badly centered,minimum width=2em}}
\tikzset{input/.style={basic,circle}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=white}}
\newcommand{\addsymbol}{\draw[thick] (0.5em,0.5em) -- (0,0.5em) -- 
                        (0,-0.5em) --  (-0.5em,-0.5em)
                        (0em,0.75em) -- (0em,-0.75em)
                        (0.75em,0em) -- (-0.75em,0em);}
\begin{figure}
    \centering
    % https://tex.stackexchange.com/a/104376
    \begin{tikzpicture}
        \node[functions] (center) {};
        \draw[thick] (0.5em,0.5em) -- (0,0.5em) -- (0,-0.5em) -- (-0.5em,-0.5em);
        \draw (0em,0.75em) -- (0em,-0.75em);
        \draw (0.75em,0em) -- (-0.75em,0em);
        \node[right of=center] (right) {};
            \path[draw,->] (center) -- (right);
        \node[functions,left=3em of center] (left) {$\sum$};
            \path[draw,->] (left) -- (center);
        \node[weights,left=3em of left] (2) {$w_2$} -- (2) node[input,left of=2] (l2) {$x_2$};
            \path[draw,->] (l2) -- (2);
            \path[draw,->] (2) -- (left);
        \node[below of=2] (dots) {$\vdots$} -- (dots) node[left of=dots] (ldots) {$\vdots$};
        \node[weights,below of=dots] (n) {$w_n$} -- (n) node[input,left of=n] (ln) {$x_n$};
            \path[draw,->] (ln) -- (n);
            \path[draw,->] (n) -- (left);
        \node[weights,above of=2] (1) {$w_1$} -- (1) node[input,left of=1] (l1) {$x_1$};
            \path[draw,->] (l1) -- (1);
            \path[draw,->] (1) -- (left);
        \node[weights,below of=left] (0) {$b$};
            \path[draw,->] (0) -- (left);
    \end{tikzpicture}
    \caption{Visualization of a perceptron. The inputs $x_i$ are multiplied with their corresponding weights $w_i$. The results are summed up and a bias is added. This is followed by the application of a threshold function. }
    \label{fig:Foundations:ANNs:Perceptron}
\end{figure}

A perceptron computes its output by multiplying each input with its corresponding weight. The results of the multiplications are then summed and compared to a threshold value. \autoref{eq:Perceptron} shows this computation.

\begin{equation}
    output = 
    \begin{cases}\label{eq:Perceptron}
    0 & if:\; \sum_{1}^{n}(w_i \cdot x_i) + b \le 0 \\
    1 & if:\; \sum_{1}^{n}(w_i \cdot x_i) + b > 0
    \end{cases}
    \text{.}
\end{equation}

The equation can be simplified by denoting ($x_1, \cdots, x_n$) and ($w_1, \cdots, w_n$) as vectors $\vec{x}$ and $\vec{y}$:

\begin{equation}
    output = 
    \begin{cases}\label{eq:PerceptronVector}
    0 & if:\; \vec{w} \cdot \vec{x} + b \le 0 \\
    1 & if:\; \vec{w} \cdot \vec{x} + b > 0
    \end{cases} \text{.}
\end{equation}

Today, \acp{nn} do not use classical perceptrons. Instead of applying a threshold function to $\vec{w} \cdot \vec{x} + b$ the neurons apply other non-linear activation functions. This gives neural networks the capability to approximate non-linear functions \cite{cybenko1989approximation, hornik1991approximation, sharma2020activation}.

\subsubsection{Activation Function}
\label{sec:Foundations:NeuralNetworks:ActivationFunction}

One such function is the sigmoid function. It has an advantage over a threshold function as small changes in the input also only produce small changes in the output \cite{nielsen2015neural}. The sigmoid function is described by \begin{equation} f(x) = \frac{1}{e^{-x}} \text{.} \end{equation} 

Another widely used activation function is the \ac{relu} \cite{fukushima1969visual, goodfellow2016deep, nielsen2015neural}. It is defined by projecting all negative values to zero. Otherwise, it returns the identity for positive values. Therefore, \ac{relu} can be expressed as \begin{equation} f(x)  = max(x,0) \text{.} \end{equation}
The cut-off below zero makes this function non-linear, thereby giving a neural network more expressive power. Moreover, cutting off negative values means that the number of active neurons at the same time is reduced which makes \ac{relu} more efficient \cite{sharma2020activation}.

Another function that is commonly used is softmax \cite{liu2016computer, goodfellow2016deep}. It is used in the last layers of classification to normalize output values and make them interpretable as probabilities over all classes. The exponential function is applied across the whole layer instead of each neuron of a layer individually. The resulting values are normalized by dividing them through the sum of all values. Softmax is described by:

\begin{equation}
f(\vec{x}) = \frac{1}{\sum_{x' \in \vec{x}} e^{x'}} 
    \begin{pmatrix}
        e^{x_1}\\
        \vdots \\
        e^{x_{n}}
    \end{pmatrix} \text{,}
\end{equation}
whereby $n$ is the number of dimensions of $\vec{x}$ and $x_i$ is the value of vector $\vec{x}$ at position $i$.   

\subsubsection{Layer}
\label{sec:Foundations:NeuralNetworks:Layer}

Neurons and their corresponding activation functions are combined into layers. The first layer is called the input layer and the last layer the output layer \cite{nielsen2015neural}. Layers in between are referred to as hidden layers. In \acp{fc} each neuron of each layer is connected to each neuron of the previous layer. This means that the output of the previous layer is feed into the next layer as input. The inputs of one individual neuron of a layer is the output of all neurons of the previous layer. Layers that have this property are commonly called fully connected layers \cite{nielsen2015neural}. 

Commonly, fully connected neural networks are shown as a graph (see \autoref{fig:Foundations:ANNs:Layer:FullyConnected}), however, mathematically, each layer is represented as a matrix of weights $\mat{W}$ and a vector of biasses $\vec{b}$. Each row vector $\vec{w_i}$ of the matrix corresponds to one neuron and the scalar $b_i$ constitutes the bias for neuron $i$. When processing input $\vec{x}$ the weight matrix is multiplied with the input vector, the bias vector $\vec{b}$ is then added, and, lastly, an activation function $f$ is applied (see \autoref{eq:FCNNLayer}).

\begin{equation}
    \label{eq:FCNNLayer}
    output = f(\mat{W} \cdot \vec{x} + \vec{b}) = f( \begin{pmatrix}
        \vec{w_1} \cdot \vec{x} + b_1\\
        \vdots \\
        \vec{w_n} \cdot \vec{x} + b_n
      \end{pmatrix} )
\end{equation}

% Adapted from https://tex.stackexchange.com/a/618305
\tikzset{>=latex} % for LaTeX arrow head
\tikzstyle{node}=[basic,circle,minimum size=22,inner sep=0.5,outer sep=0.6]
\tikzstyle{connect}=[->,basic,black,shorten >=1]
\tikzset{ % node styles, numbered for easy mapping with \nstyle
  node 1/.style={node,black,draw=black,fill=white},
  node 2/.style={node,black,draw=black,fill=white},
  node 3/.style={node,black,draw=black,fill=white},
}
\def\nstyle{int(\lay<\Nnodlen?min(2,\lay):3)} % map layer number onto 1, 2, or 3

\begin{figure}
    \centering
    % NEURAL NETWORK
    \begin{tikzpicture}[x=2.4cm,y=1.2cm]
        \readlist\Nnod{4,3,3,2} % array of number of nodes per layer
        \readlist\Nstr{n,m,l} % array of string number of nodes per layer
        \readlist\Cstr{x,h^{(\prev)},y} % array of coefficient symbol per layer
        \def\yshift{0.55} % shift last node for dots
        
        % LOOP over LAYERS
        \foreachitem \N \in \Nnod{
        \def\lay{\Ncnt} % alias of index of current layer
        \pgfmathsetmacro\prev{int(\Ncnt-1)} % number of previous layer
        \foreach \i [evaluate={\c=int(\i==\N); \y=\N/2-\i-\c*\yshift;
                    \x=\lay; \n=\nstyle;
                    \index=(\i<\N?int(\i):"\Nstr[\n]");}] in {1,...,\N}{ % loop over nodes
            % NODES
            \node[node \n] (N\lay-\i) at (\x,\y) {$\strut\Cstr[\n]_{\index}$};
            
            % CONNECTIONS
            \ifnumcomp{\lay}{>}{1}{ % connect to previous layer
            \foreach \j in {1,...,\Nnod[\prev]}{ % loop over nodes in previous layer
                \draw[white,line width=1.2,shorten >=1] (N\prev-\j) -- (N\lay-\i);
                \draw[connect] (N\prev-\j) -- (N\lay-\i);
            }
            \ifnum \lay=\Nnodlen
                \draw[connect] (N\lay-\i) --++ (0.5,0); % arrows out
            \fi
            }{
            \draw[connect] (0.5,\y) -- (N\lay-\i); % arrows in
            }
            
        }
        \path (N\lay-\N) --++ (0,1+\yshift) node[midway] {$\vdots$}; % dots
        }      
    \end{tikzpicture}
    \caption{Visualization of simple \acl{fc} with two hidden layers with $m$, an input layer with $n$ and an output layer with $l$ neurons. The neurons of the input, hidden and output layers are denoted with $x_i$, $h_j^{(a)}$ and $y_k$, respectively. The output of each neuron is feed into every neuron of the following layer.}
    \label{fig:Foundations:ANNs:Layer:FullyConnected}
\end{figure}

\subsubsection{Training Neural Networks}
\label{sec:Foundations:NeuralNetworks:Training}

To train a neural network training samples are shown to a network \cite{nielsen2015neural}. The network uses the samples to make a prediction. Based on a loss function the values are compared to the target prediction and an optimizer updates the models weights. This process is repeated until the model stops improving.


\paragraph{Loss Function}
\label{par:Foundations:NeuralNetworks:Training:Loss}

The loss function is a function that indicates how well a neural network is performing. Minimizing the loss is the goal during training \cite{teuwen2020convolutional}. To minimize the loss backpropagation is used \cite{nielsen2015neural}. The gradient of the loss function is computed with respect to the weights. This is done by starting at the output layer and propagating the error back through the network, calculating the gradient along the way. The type of problem dictates which loss function can be used. Relevant for this thesis are loss functions that work for regression problems. One common metric is \ac{mse} \cite{goodfellow2016deep}. It is described as the mean of squared differences between predictions and targets. This is described by \begin{equation} MSE = \frac{1}{N} \sum_{i=1}^N (p_i - t_i)^2 \text{,} \end{equation} with $N$ being the number of predictions and $p_i$ the predicted value for target $t_i$.

\Ac{mse} punishes predictions which are further off from a target disproportionally strong, thereby incentivizing minimization of distance across all predictions.


\paragraph{Optimizer}
\label{par:Foundations:NeuralNetworks:Training:Optimizer}

With the computed gradient the optimizer decides how the weights of a model get updated to reduce the average error of the loss function on any observation $z$.

Gradient descent is one such optimizer and uses the gradient $\Delta \mathcal{L}(z_{train})$ based on all observation in the training data $z_{train}$ to estimate the true gradient $\mathcal{L}(z)$. This and a learning rate parameter $\eta$ is used to update the weights $w$ to $\bar{w}$ \cite{murphy2022probabilistic}. The formula that describes gradient descent is as follows:

\begin{equation} \bar{w} = w - \eta \cdot \Delta \mathcal{L}(z_{train}). \end{equation}

Often the training data is large, thus calculating $\Delta \mathcal{L}(z_{train})$ takes a long time. To combat that, in \ac{sgd}, the training data is split into mini-batches $z_{b}$ which are used to estimate $\Delta \mathcal{L}(z_{train})$. This results in a changed training formula:
\begin{equation} \bar{w} = w - \eta \cdot \Delta \mathcal{L}(z_{b}). \end{equation} This allows more frequent weight updates and, therefore, results in faster training time.


Instead of using the gradient directly, another optimizer: \ac{adam} \cite{kingma2017adam}, uses the decaying average $m_t$ and squared average $v_t$ of past gradients, thus preserving momentum from gradients of previous batches. \Ac{adam} has two decay rate parameters $\beta_1$ and $\beta_2$ for $m_t$ and $v_t$ respectively. \textcite{kingma2017adam} also introduce the bias-corrected versions $\hat{m_t}$ and $\hat{v_t}$. \Ac{adam} defines parameters in terms of the current time step $t$ and the previous time step with $t-1$. The first moment estimate $m_t$ is computed by $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot \Delta \mathcal{L}(z_b)$. Similarly, the second moment estimate $v_t$ is computed by $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot (\Delta \mathcal{L}(z_b))^2$. 
The bias corrected-versions of the moments are computed by $\hat{m_t} = \frac{m_t}{1 - \beta_1^t}$ and $\hat{v_t} = \frac{v_t}{1 - \beta_2^t}$. These formulas allow the construction of the update equation which looks as follows:

\begin{equation}w_t = w_{t-1} - \eta \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon} \text{.} \end{equation}

The small constant $\epsilon$ is used to prevent division by zero. \textcite{kingma2017adam} state that \ac{adam} is an optimizer that is fairly robust to noise.


\subsection{Convolutional Neural Networks}
\label{sec:Foundations:NeuralNetworks:CNN}

\Acp{cnn} are feedforward neural networks; however, instead of only using fully connected layers, \acp{cnn} use filters or convolutions \cite{teuwen2020convolutional}. Convolutions or filters consist of a filter matrix which can be understood as a sliding window. The values of the convolution matrix are multiplied with their corresponding pixel values and summed (see \autoref{fig:Foundations:ANNs:CNNs:Convolution}). Convolutions stem from traditional image processing \cite{shih2010image} and are used because they take the spacial structure of images directly into account \cite{nielsen2015neural}. Instead of having predefined filter matrices convolutional neural networks learn their values during training.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/20_foundations/30_neural_networks/convolution.eps}
    \caption{Visualization of a convolutional (Laplace) filter. The filter uses represents the first order approximation of the Laplacian. The pixel values are multiplied with corresponding values in the filter and then summed up to get the final result. }
    \label{fig:Foundations:ANNs:CNNs:Convolution}
\end{figure}


\subsection{Architecture}
\label{sec:Foundations:NeuralNetworks:Architecture}

Large neural networks use similar patterns, and, therefore, architectural building blocks exist. The building blocks relevant to this thesis are introduced in this section.
Of note is also that architecture descriptions often describe activation functions as separate layers as can be seen in \autoref{fig:Foundations:ANNs:Architecture:ResNet:Block}.

A layer type that can be commonly found in neural networks is the fully connected layer. In this layer each input is connected to each neuron. This is the layer-type used in fully connected \ac{nn} as introduced in \autoref{sec:Foundations:NeuralNetworks:FNN}.

Also, \emph{convolutional layers}, were already introduced in \autoref{sec:Foundations:NeuralNetworks:CNN}. These layers use convolutions and, therefore, each neuron is only connected to its neighbours. What is considered to be its neighbours depends on the size of the convolution window. This size is the kernel size. The example shown in \autoref{fig:Foundations:ANNs:CNNs:Convolution} has a kernel size of 3 (sometimes also described as $3 \times 3$). The output channels of a layer define the number of different kernels which are used inside a layer. Each kernel is run on the entire input.

Closely related to convolutional layers are \emph{pooling layers}. However, unlike convolutions, pooling layers do not perform a sum operation on the inputs. Instead, they apply a pooling function (often, maximum or mean). Also, commonly with pooling layers the sliding window does not overlap ($stride = \frac{size(kernel)}{2}$) \cite{scherer2010evaluation}, therefore, pooling can be understood as splitting the input into tiles and a pooling operation is done on each of those tiles. Pooling is used to aggregate neighbouring low-level features and to reduce the spatial size of data and therefore require fewer weights in a network.

\emph{Global average pooling} follows the same concept of pooling layers, however, the goal with it is to generate a fixed size output that is independent of the input size. This allows to replace fully connected layers in neural networks \cite{lin2014network}. The fixed output size makes models input size agnostic and can be used to also introduce fully connected layers without sacrificing this flexibility in fully connected networks \cite{he2016deep}.

\emph{Batch normalization} \cite{ioffe2015batch} is a layer only used during training (not during validation and testing) that normalizes the output $y$ of a layer before feeding it to the activation function. The normalization is done over a whole mini-batch. The mean is subtracted from the output and then a division by the \acf{std} is made: $\frac{y - mean(y)}{std (y)}$. It allows a higher learning rate and less careful initialization, thereby, reducing the time it takes to train a network.


\subsubsection{\acs{resnet}}
\label{sec:Foundations:NeuralNetworks:Architecture:ResNet}

\Ac{resnet} was introduced by \textcite{he2016deep} with the goal of training networks with more layers. Commonly, \acp{resnet} are denoted as \acs{resnet}-N where N is the number of layers. \Ac{resnet} is built by chaining residual blocks. The last layers are a global average pooling layer followed by a fully-connected layer.

A residual block consists of multiple convolutional layers followed by a \ac{batch-norm} layer and a \ac{relu} activation function. The last layer, however, adds the original input to its output before applying \ac{relu} (see \autoref{fig:Foundations:ANNs:Architecture:ResNet:Block}).

\tikzset{layer/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=white}}
\begin{figure}
    \centering
    \begin{tikzpicture}[rotate=90,transform shape]
        \node[layer] (input) {x};
        \node[layer, below of=input] (conv1) {convolutional layer};
        \node[layer, below of=conv1] (bn1) {\acl{batch-norm}};
        \node[layer, below of=bn1] (relu1) {\acs{relu}};
        \node[layer, below of=relu1] (conv2) {convolutional layer};
        \node[layer, below of=conv2] (bn2) {\acl{batch-norm}};
        \node[functions, below of=bn2] (plus) {+};
        \node[layer, below of=plus] (relu2) {\acs{relu}};
        \node[below of=relu2] (output) {};

        \path[draw,->] (input) -- (conv1);
        \path[draw,->] (conv1) -- (bn1);
        \path[draw,->] (bn1) -- (relu1);
        \path[draw,->] (relu1) -- (conv2);
        \path[draw,->] (conv2) -- (bn2);
        \path[draw,->] (bn2) -- (plus);
        \path[draw,->] (plus) -- (relu2);
        % add intermitten point: [out=0,in=90] ($(relu1)+(2.5,0)$) to [out=-90, in=0 ]
        \path[draw,->] (input) to [out=0,in=90] ($(relu1)+(2.5,0)$) to [out=-90, in=0 ] (plus);
        \path[draw,->] (relu2) to (output);
        
    \end{tikzpicture}
    \caption{Architecture of a residual block in \acs*{resnet}. The input $x$ is feed through a convolutional layer, then through a \acl{batch-norm} layer. This is followed by the application of a \ac{relu} function. Afterwards, another convolutional and \acl{batch-norm} are added. The result is added to the original input and feed to a final \ac{relu} function.}
    \label{fig:Foundations:ANNs:Architecture:ResNet:Block}
\end{figure}


\subsubsection{Inception}
\label{sec:Foundations:NeuralNetworks:Architecture:Inception}

The Inception v3 architecture, introduced by \textcite{szegedy2016rethinking}, works by the use of inception modules. These modules compute convolutions of different sizes in parallel and concatenate the results. Inception v3 uses three types of inception modules that slightly vary in architecture, but all follow the same concept. The original naive inception module uses one $1 \times 1$, one $3 \times 3$ and one $5 \times 5$ convolutional layer. Moreover, one $3 \times 3$ max pooling layer is also present. This architecture is shown in \autoref{fig:Foundations:ANNs:Architecture:Inception:Naive}.

\tikzset{layer/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle,fill=white}}
\begin{figure}
    \centering
    \begin{tikzpicture}[transform shape]
        \node[layer] (input) {x};
        \node[layer, below = of input, text width=1.8cm,align=center] (conv3) {convolution ($3 \times 3$)};
        \node[layer, below = of conv3] (relu3) {\acs{relu}};

        \node[layer, left = of conv3, text width=1.8cm,align=center] (conv1) {convolution ($1 \times 1$)};
        \node[layer, below = of conv1] (relu1) {\acs{relu}};

        \node[layer, right = of conv3, text width=1.8cm,align=center] (conv5) {convolution ($5 \times 5$)};
        \node[layer, below = of conv5] (relu5) {\acs{relu}};

        \node[layer, right = of conv5, text width=1.5cm,align=center] (max3) {max pool ($3 \times 3$)};
        \node[layer, below = of max3] (reluM3) {\acs{relu}};

        \node[functions, below = of relu3] (concat) {concat};
        \node[below = of concat] (output) {};

        \path[draw,->] (input) -- (conv3);
        \path[draw,->] (input) -- (conv1);
        \path[draw,->] (input) -- (conv5);
        \path[draw,->] (input) -- (max3);


        \path[draw,->] (conv3) -- (relu3);
        \path[draw,->] (conv1) -- (relu1);
        \path[draw,->] (conv5) -- (relu5);
        \path[draw,->] (max3)  -- (reluM3);


        \path[draw,->] (relu3)  -- (concat);
        \path[draw,->] (relu1)  -- (concat);
        \path[draw,->] (relu5)  -- (concat);
        \path[draw,->] (reluM3) -- (concat);

        \path[draw,->] (concat) to (output);
        
    \end{tikzpicture}
    \caption{The architecture of a naive inception module \cite{szegedy2015going}. The input is feed into four layers, three of which are convolutional layers of sizes $1 \times 1$, $3 \times 3$, $5 \times 5$, and one $3 \times 3$ max pooling layer, simuditltaneously. Each layers output is feed through a \ac{relu} activation function and the output of all is concatenated.}
    \label{fig:Foundations:ANNs:Architecture:Inception:Naive}
\end{figure}


\section{Related Works}
\label{sec:Foundations:RelatedWorks}

This section presents related work to this thesis. First, traditional focus metrics are introduced that do not rely on machine learning. Second, machine learning approaches are introduced. Unlike this thesis, these approaches are using tissue samples and not stool samples. Moreover, they do not encode distance or directional information.

\subsection{Traditional Methods}
\label{sec:Foundations:RelatedWorks:Traditional}


\textcite{mateos-perez2012comparative} perform a comparative study on 13 real-time focus methods with fluorescence-labeled tuberculosis bacteria. They take into account, robustness to noise, illumination changes, accuracy and computational cost. Further, the study looks into the benefit of preprocessing with morphological operators.
The authors recommend the usage of \ac{mdct} (studied by \textcite{lee2008enhanced}) and \ac{vol4} \cite{vollath1988influence} due to their high accuracy, robustness to noise and low computational cost.

\ac{vol4} works by taking the difference of the summed values of a multiplication with neighbouring pixels. The computation is shown in \autoref{eq:Vol4}, where $g(x, y)$ is the grey-level of the image at position $x,y$, $M$ is the width and $N$ the height of the image.

\begin{equation}
    \label{eq:Vol4}
    F_{Vol4} = \sum_{i=1}^{M-1}\sum_{j=1}^{N} g(i,j) \cdot g(i+1, j) - \sum_{i=1}^{M-2}\sum_{j=1}^{N} g(i,j) \cdot g(i+2, j)
\end{equation}


\Ac{mdct} (originally named \emph{MF-DCT}) on the other hand, was proposed by \textcite{lee2008enhanced} and uses a $4 \times 4$ convolutional filter. The focus measure is then the squared sum of the applied convolutional operation on the whole image. The corresponding formula is shown in \autoref{eq:MDCT}.

\begin{equation}
    \label{eq:MDCT}
    F_{MDCT}= \sum_{m=1}^{M}\sum_{n=1}^{N}
    ( g(m,n) *
    \left[ {\begin{array}{rrrr}
        1 & 1 & -1 & -1 \\
        1 & 1 & -1 & -1 \\
        -1 & -1 & 1 & 1 \\
        -1 & -1 & 1 & 1
    \end{array} } \right]
    )^2
\end{equation}

The approach that is currently used in the microscope prototype is the mean Laplacian. The Laplacian is an edge detection convolutional filter \cite{vikrammutneja2015methods}. The result can be found in \autoref{eq:MeanLaplacian}. \textcite{redondo2012autofocus} look at a similar function, however, they instead use the sum of the squared Laplacian.

\begin{equation}
    \label{eq:MeanLaplacian}
    F_{ML}= \frac{1}{NM} \sum_{m=1}^{M}\sum_{n=1}^{N} (g(m,n) *
    \left[ {\begin{array}{rrr}
        1 & 1 & 1 \\
        1 & -8 & 1 \\
        1 & 1 & 1 
    \end{array} } \right]
    )
\end{equation}

\subsection{Deep Learning-Based Approaches}
\label{sec:Foundations:RelatedWorks:DeepLearning}

Related literature for out-of-focus detection that uses deep learning mostly uses pathology slides. Further, magnification levels of microscopes range from 20x to 40x, with the latter being most common. Moreover, commonly, the problem is phrased as a classification problem of either, \ac{oof} classes or simple binary classes.

\emph{DeepFocus} is a model developed in \textcite{senaras2018deepfocus} to classify an image tile as either in- or out-of-focus. It is the first deep learning model for focus-detection for the use in digital pathology. The training data used is generated by annotating the focus plane of focus stacks. Images that are annotated as in-focus and adjacent images in the stack are considered to be in focus. The model is a \ac{cnn} with five convolutional layers, three max pooling and two fully connected layers. The resulting classification model is compared to out-of-focus detection by \textcite{moleslopez2013automated} who use a non-neural network machine learning approach with decision trees.
Additionally, the authors compare the computational performance of their approach with \textcite{moleslopez2013automated}. \emph{DeepFocus} performs slightly faster than \textcite{moleslopez2013automated} and offers a higher resolution. This approach allows GPU computations which can be used to further speed-up out of focus-detections.
The authors conclude that their approach provides good, spacial resolution and accuracy for identifying \ac{oof} regions in pathology slides. Moreover, they state that the use of GPUs allows faster processing times on ordinary hardware.

\textcite{kohlberger2019wholeslide} develop \emph{ConvFocus}, a truncated Inception (v3) model that classifies pathology slides based on how strongly out of focus they are. For this approach 30 different classes are used of which one class is considered to be in focus. The aim of this study is to use in-focus images and artificially generate out of focus training data.
The authors find that exclusively using Bokeh or Gaussian-blur for synthetic \ac{oof} images yields mediocre detection results. They indicate that artificial blurring is removing artefacts caused by edges of scan lanes and JPEG compression. Therefore, the authors re-add these types of artefacts after synthetic image blurring. They, introduced additional pixel noise to simulate the noise of image sensor.
The main findings of the authors show that their developed method can be used to generate additional data for training focus classifiers. Additionally, the authors show the effect of synthetic blur on detection accuracy of breast cancer prediction models and find that \ac{oof} images degrade performance.

An approach with a slightly different goal is followed by \textcite{wang2020focuslitenn} to develop \emph{FocusLite}. \emph{FocusLite} detect the degree to which images are out of focus but does not discern any directional information. The authors' goal is to develop a model which maintains similar speeds to knowledge-driven approaches, yet, surpasses their precision. The models are trained on \emph{FocusPath} \cite{hosseini2019encoding} and evaluated using a newly created dataset \emph{TCGA@Focus} and \emph{FocusPath} itself.
The authors conclude that their model surpasses all existing methods when considering the trade-off between precision and speed.  
