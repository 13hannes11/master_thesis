\chapter{Methods}
\label{ch:Methods}

\todo[inline, caption={}]{
    \begin{itemize}
        \item Goal of this Chapter: A description of how the thesis was carried out so that the reader can.
        \item The method section should answer two key questions:
        \begin{itemize}
            \item What did you do?
            \item Why did you do it this way?
        \end{itemize}
        \item Procedure: Describes steps carried out in project.
        \item Meterials: Lists hardware (equipment, apparatuses), or software (programs) used to carry out the project.
        \item Definitions: Presents terminology that needs to be specifically defined for the context of this project.
    \end{itemize}
}

\section{Data}
\label{sec:Methods:Data}

At the time of writing there is no available dataset for Kato-Katz prepared slides, especially, not of \acp{ntd}. A large dataset of annotated slides which contains annotations for bounding boxes and the type of eggs is available at Etteplan. This dataset is subdivided into different studies and study 31 was used for this thesis. \todo{Add some information about the contents and origin of the study also add information about the microscopes.} Slides with verified annotations were used and split into tiles of $150 \times 150$ pixels. Only tiles which area is covered by a bounding box to at least 25 percent are kept. \autoref{fig:Methods:Data:PatchCreation} illustrates which tiles are kept for an example annotation. The tile size was chosen due to smaller tile sizes being more difficult to annotate.

\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{images/30_methods/10_data/patch_creation.pdf}
    \caption{Visualization of patch creation. The two patches (with a blue border) at the bottom of the page have sufficient overlap while the ones at the top (with a grey border) do not. The rectangle in the centre (with a red border) represents an actual bounding box of an egg.}
    \label{fig:Methods:Data:PatchCreation}
\end{figure}

\todo[inline]{add something about focus values, what they mean and magnitude}

\subsection{Annotation}
\label{sec:Methods:Data:Annotation}

To annotate the focus plane of these focus stacks the annotation tool \emph{Focus Annotator} \cite{kuchelmeister2022focus} was developed (see \autoref{fig:Methods:Data:FocusAnnotator}). The tool allows quick annotations using keyboard shortcuts. It, moreover, features a grid view to show neighbouring tiles and give contextual information.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/30_methods/10_data/focus_annotator.png}
    \caption{A screenshot of the tool \emph{Focus Annotator} showing a tile and its neighbours that contain a schistosoma egg.}
    \label{fig:Methods:Data:FocusAnnotator}
\end{figure}


Approximately 100 focus stack tiles (i.e., 108 schistosoma, 104 hookworm, 121 trichuris, 104 ascaris) were annotated per egg type. The annotation of a focus stack is used to calculate the relative focus distance (as lens parameter) to the most in focus patch. The difference in lens value between neighbouring images of a stack is approximately $0.0068$.  One stack consists of 10 images and therefore this resulted in 4367 annotated patches. These were split using a $75\%$, $15\%$, $15\%$ split for training validation and test data respectively. Background images were not chosen as for egg counting it is not important to have the background in focus.

\subsection{Augmentation}
\label{sec:Methods:Data:Augmentation}

The images are transformed using PyTorch's transformations \cite{paszke2019pytorch, 2021pytorch}. \texttt{ToTensor} converts an image's value range to the interval $[0, 1]$ and \texttt{ConvertImageDtype} is used to ensure that an image consists of 32-bit floating point numbers. Further, images used for training (not for validation and testing) are augmented by using vertical and horizontal flips, and rotations. Each augmentation has a 50 percent chance of being applied. Rotations are either 90, 180 or 270 degrees. All augmentations do not require interpolation, therefore, the sharpness of the image is not changed. This is done to make trained models more robust towards changes in direction. Further, it can improve model performance when only limited training data is available \cite{shorten2019survey}.


\subsection{Datasets in Related Work}
\label{sec:Methods:Data:Related}

Related studies use 20 to 40 times the amount of data to train their models (see \autoref{tab:Methods:Data:Comparison}). \emph{FocusLiteNN} understfates the amount of data due to the usage of larger patches than the model's input size. With a patch size of $1024 \times 1024$, an input size of $235 \times 235$ and a stride of 128, the effective amount of data can be assumed to be around 10 to 50 times higher.


\renewcommand{\thefootnote}{\alph{footnote}} % change footnote to letters
\begin{table}[ht]
    \centering
    \caption{Comparison of datasets used for each model.}
    \begin{tabular}{| c | r r r r |} 
        \hline
        model & magnification & pixel size (\micro\meter) & input size &  dataset size\\
        \hline
        \textbf{\acs{poop}} & $10$ & $0.96$\phantom{$^1$} & $150^2$ & $4367$\phantom{$^1$}\\
        \hline
        DeepFocus & $40$ & $0.25$\phantom{$^1$} & $64^2$ & $216 000$\phantom{$^1$}\\ 
        ConvFocus & $40$ & $0.24$\tablefootnote{The size ranges from \unit{0.227}{\micro\meter} to \unit{0.251}{\micro\meter}.} 
        & $139^2$ & $166 000$\tablefootnote{The number includes only in-focus patches.}\\
        FocusLiteNN & $20$ & $0.50$\phantom{$^1$} & $235^2$ & $8 640$\tablefootnote{The actual number is substantially larger due to sampling (stride $128$) of $1024 \times 1024$ tiles.}\\
        \hline
    \end{tabular}
    \label{tab:Methods:Data:Comparison}
\end{table}
\renewcommand{\thefootnote}{\arabic{footnote}} % change footnote back to numbers

\section{Models}
\label{sec:Methods:Models}


\section{Training}
\label{sec:Methods:Training}

\todo{mention Hyperparameter tuning}
Each model is trained for 100 epochs to ensure convergence and the best model (based on validation).

As a loss function mean squared error is used (punishing predictions that are further off more strongly) \todo{citation needed?!}.

Conv and FullyConnected were trained using separate runs of Optuna \todo{reference}. In these runs the architecture and hyperparameters were optimized.

\Ac{resnet} models were trained using a shared run of the Optuna \todo{reference} optimizer. The best performing models of this run were chosen to be presented. Hyperparameter tuning was used

\todo[inline]{mention transfer learning}

\section{Evaluation}
\label{sec:Methods:Evaluation}

\subsection{Metrics}
\label{sec:Methods:Evaluation:Metrics}

The trained models are evaluated using multiple metrics that are described in this section.

The model's trained in this thesis are regression models. The evaluation and comparison, therefore, the used metrics relate regression values. For the computations pytorchmetrics \todo{reference} is used. The first metric is \ac{mae} \todo{add reference}. \Ac{mae} is computed by taking the mean of the absolute difference between target and prediction values. Its formula is $$E_{MAE} = \frac{1}{N}\sum_{i=1}^N |p_i - t_i| \text{,}$$ where $N$ is the number of predictions (and targets) and $p_i$ is the predicted value for target $t_i$.
It is used to provide a simple link between lens values and model performance. It indicates how far off the model prediction are compared to the values of a focus stack.

The \ac{plcc} \cite{pearson1909determination} is a correlation measure used as another metric to measures if relationship between two variables is linear \cite{profillidis2019statistical}. A linear relationship is expected between the actual focus distance and the prediction of model. It is defined as:

$$\acs{plcc} = \frac{cov(X,Y)}{std(X) \cdot std(Y)} \texttt{,}$$
with $cov$ being the covariance and $std$ being the standard deviation. The value range of \ac{plcc} ranges from $-1$ to $1$ and according to \textcite{profillidis2019statistical} $\acs{plcc} = 1$ means a perfect positive correlation, $\ac{plcc} > 0.8$ a strong correlation. A $\ac{plcc} = 0$ means no correlation at all and values below zero indicate a negative correlation with $\ac{plcc} = -1$ being a perfectly negative correlation.


The \ac{srcc} \cite{spearman1904proof} is related to the \ac{plcc}, however it uses the rank of variables instead of their actual value. Therefore, it describes the relationship between two variables using monotonic function \todo{source}. The formula that describes \ac{srcc} looks as follows:

$$\acs{srcc} = \frac{cov(R(X),R(Y))}{std(R(X)) \cdot std(R(Y))} \texttt{,}$$

with $R$ the rank of variables, $std$ the standard deviation and $cov$ being the covariance.

Lastly, the \ac{r2} is a common statistical correlation measure in regression analysis \cite[dibucchianico2008coefficient].

$$\acs{r2} = 1 - \frac{\sum_{i=1}^{N}{(p_i - t_i)^2}}{\sum_{i=1}^{N}{(p_i - \bar{t})^2}} \text{,}$$ with $\bar{t}$ being the mean of all annotations.

The value range of \acs{r2} is between 0 and 1. According to \textcite{dibucchianico2008coefficient} values lower than $<0.5$ show only a weak relation between data and model, while values $v$ with $0.5 < v < 0.8$ demonstrate that a model is not adequate to perfectly represent the data.


The output of the models developed in this thesis is not directly comparable to other approaches. Therefore, the test data is grouped into focus stacks. Each model is run on all patches of a focus stack. The image with the highest focus score assigned, is considered to be the model's prediction. The prediction is considered to be correct if the in-focus annotation matches the prediction. A prediction that is off by one is still considered to be correct (see \autoref{fig:Methods:Data:InFocusExampleImages}).
The reasoning behind this approach is that during the annotation process it was difficult for annotators to decide on which image is most in focus compared to its neighbours. The same approach is undertaken by \citeauthor{senaras2018deepfocus} (DeepFocus) \cite{senaras2018deepfocus} (see \autoref{sec:Foundations:RelatedWorks:DeepLearning}). To get further understanding of the magnitude of a model's misprediction the \ac{mae} of the index is used. In this case the formula for MAE looks as follows:

$$E_{MAE^*} = \frac{1}{|S|}\sum_{s \in S} |p(s) - t(s)| \text{,}$$ with $S$ being the set of evaluated focus stacks, $p(s)$ denoting the index of the image predicted to be in focus and $t(s)$ denoting the index of the image that is annotated as in-focus.

To distinguish between \ac{mae} metrics used in this thesis the index based metric is denoted as \ac{mae}$^*$.

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/+0.0068_+1_stack.jpg}
        \caption{+1}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/+0.0_stack.jpg}
        \caption{in focus}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/-0.0068_-1_stack.jpg}
        \caption{-1}
    \end{subfigure}

    \caption{Figure showing which images are considered to be in focus for the best image selection.}
    \label{fig:Methods:Data:InFocusExampleImages}
\end{figure}


\subsection{Implementation of Traditional Focusing Methods}
\label{sec:Methods:Evaluation:Traditional}

Traditional focusing methods (introduced in \autoref{sec:Foundations:RelatedWorks:Traditional}) were implemented using \emph{PyTorch} \cite{paszke2019pytorch} and \emph{Kornia} \cite{riba2020kornia}. Kornia is a computer vision library that uses PyTorch as its computational backend. Therefore, computations can be easily run on the \ac{cpu} or \ac{gpu}. In this section the PyTorch functions \texttt{torch.sum}, \texttt{torch.mul}, \texttt{torch.tensor}, \texttt{torch.square}, and \texttt{torch.mean} are shortened by omitting the namespaces of the functions. The same is done for \texttt{kornia.filters.filter2d} and \texttt{kornia.filters.laplacian}. In the code shown below \texttt{IMG} is a tensor representing the greyscale version of the input image.

\Ac{vol4} was implemented using PyTorch tensors, their index operations and the tensor operations \texttt{sum} and \texttt{mul}. The slice indexing removes the need to run the summation loop in Python and therefore allows PyTorch's efficient computation backends to be used.

\begin{python}
VOL4 = sum(mul(IMG(:-1,:), IMG(1:,:)))      \
        - sum(mul(IMG(:-2,:), IMG(2:,:)))
\end{python}


\Ac{mdct} was implemented using Kornia's \texttt{filter2d} function in combination with \texttt{tensor}, \texttt{sum} and \texttt{square} from PyTorch. The shortened code looks as follows:

\begin{python}
kernel = tensor([[[ 1,  1, -1, -1],
                  [ 1,  1, -1, -1],
                  [-1, -1,  1,  1],
                  [-1, -1,  1,  1]]])
MDCT = sum(square(filter2d(IMG, kernel)))
\end{python}

Kornia in combination with PyTorch is also used for the implementation of \ac{laplacian} (as shown below). The \texttt{laplacian} is already implemented in Kornia therefore it can be used in combination with PyTorch's \texttt{mean} function. This results in the following code.

\begin{python}
ML = mean(laplacian(IMG, 3))
\end{python}

\subsection{Computational Resource Usage}
\label{sec:Methods:Evaluation:Computation}

The speed (performance) of the models is analysed by running each method on 620 image patches. The measurement only includes computation time. Transfer time to the GPU and loading time of images is not considered.

Also, a theoretical analysis is given. Further, the memory consumption is analysed from a theoretical viewpoint. \todo{Not sure if theoretical stuff should be mentioned here or rather under results}

\todo{write how memory measurements were obtained and strice theoretical part.}

\section{Hardware and Software}
\label{sec:Methods:Hardware}

The utilized hardware for training models and running benchmarks was a machine running Ubuntu 20.04.3 with a Ryzen 5 2600X \ac{cpu}, 64 GiB of RAM, and an Nvidia GeForce RTX 2070 (8 GiB) \ac{gpu}.

\todo[inline]{potentially merge with computational resource usage}

