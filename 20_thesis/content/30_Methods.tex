\chapter{Methods}
\label{ch:Methods}

\todo[inline, caption={}]{
    \begin{itemize}
        \item Goal of this Chapter: A description of how the thesis was carried out so that the reader can.
        \item The method section should answer two key questions:
        \begin{itemize}
            \item What did you do?
            \item Why did you do it this way?
        \end{itemize}
        \item Procedure: Describes steps carried out in project.
        \item Meterials: Lists hardware (equipment, apparatuses), or software (programs) used to carry out the project.
        \item Definitions: Presents terminology that needs to be specifically defined for the context of this project.
    \end{itemize}
}

\section{Hardware and Software}
\label{sec:Methods:Hardware}

The utilized hardware for training models and running benchmarks was a machine running Ubuntu 20.04.3 with a Ryzen 5 2600X processor, 64 GiB of RAM, and an Nvidia GeForce RTX 2070 (8 GiB) graphics card.


\section{Data}
\label{sec:Methods:Data}

At the time of writing there is no available dataset for Kato-Katz prepared slides, especially, not of \acp{ntd}. A large dataset of annotated slides which contains annotations for bounding boxes and the type of eggs is available at Etteplan. This dataset is subdivided into different studies and study 31 was used for this thesis. \todo{Add some information about the contents and origin of the study also add information about the microscopes.} Slides with verified annotations were used and split into tiles of $150 \times 150$ pixels. Only tiles which area is covered by a bounding box to at least 25 percent are kept. \todo{Add visualization to illustrate point.} The tile size was chosen due to smaller tile sizes being more difficult to annotate.

\subsection{Annotation}
\label{sec:Methods:Data:Annotation}

To annotate the focus plane of these focus stacks the annotation tool \emph{Focus Annotator} \cite{kuchelmeister2022focus} was developed (see \autoref{fig:Methods:Data:FocusAnnotator}). The tool allows quick annotations using keyboard shortcuts. It, moreover, features a grid view to show neighbouring tiles and give contextual information.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/30_methods/10_data/focus_annotator.png}
    \caption{A screenshot of the tool \emph{Focus Annotator} showing a tile and its neighbours that contain a schistosoma egg.}
    \label{fig:Methods:Data:FocusAnnotator}
\end{figure}


Approximately 100 focus stack tiles (i.e., 108 schistosoma, 104 hookworm, 121 trichuris, 104 ascaris) were annotated per egg type. The annotation of a focus stack is used to calculate the relative focus distance (as lens parameter) to the most in focus patch. The difference in lens value between neighbouring images of a stack is approximately $0.0068$.  One stack consists of 10 images and therefore this resulted in 4367 annotated patches. These were split using a $75\%$, $15\%$, $15\%$ split for training validation and test data respectively. Background images were not chosen as for egg counting it is not important to have the background in focus.

\subsection{Augmentation}
\label{sec:Methods:Data:Augmentation}

The images are transformed using PyTorch's transformations \cite{paszke2019pytorch, 2021pytorch}. \texttt{ToTensor} converts an image's value range to the interval $[0, 1]$ and \texttt{ConvertImageDtype} is used to ensure that an image consists of 32-bit floating point numbers. Further, images used for training (not for validation and testing) are augmented by using vertical and horizontal flips, and rotations. Each augmentation has a 50 percent chance of being applied. Rotations are either 90, 180 or 270 degrees.
This is done to make trained models more robust towards changes in direction. Further, it can improve model performance when only limited training data is available. \todo{citation needed}


\subsection{Datasets in Related Work}
\label{sec:Methods:Data:Related}

Related studies use 20 to 40 times the amount of data to train their models (see \autoref{tab:Methods:Data:Comparison}). \emph{FocusLiteNN} understfates the amount of data due to the usage of larger patches than the model's input size. With a patch size of $1024 \times 1024$, an input size of $235 \times 235$ and a stride of 128, the effective amount of data can be assumed to be around 10 to 50 times higher.


\renewcommand{\thefootnote}{\alph{footnote}} % change footnote to letters
\begin{table}[ht]
    \centering
    \caption{Comparison of datasets used for each model.}
    \begin{tabular}{| c | r r r r |} 
        \hline
        model & magnification & pixel size (\micro\meter) & input size &  dataset size\\
        \hline
        \textbf{\acs{poop}} & $10$ & $0.96$\phantom{$^1$} & $150^2$ & $4367$\phantom{$^1$}\\
        \hline
        DeepFocus & $40$ & $0.25$\phantom{$^1$} & $64^2$ & $216 000$\phantom{$^1$}\\ 
        ConvFocus & $40$ & $0.24$\tablefootnote{The size ranges from \unit{0.227}{\micro\meter} to \unit{0.251}{\micro\meter}.} 
        & $139^2$ & $166 000$\tablefootnote{The number includes only in-focus patches.}\\
        FocusLiteNN & $20$ & $0.50$\phantom{$^1$} & $235^2$ & $8 640$\tablefootnote{The actual number is substantially larger due to sampling (stride $128$) of $1024 \times 1024$ tiles.}\\
        \hline
    \end{tabular}
    \label{tab:Methods:Data:Comparison}
\end{table}
\renewcommand{\thefootnote}{\arabic{footnote}} % change footnote back to numbers

\section{Models}
\label{sec:Methods:Models}


\section{Training}
\label{sec:Methods:Training}

\todo{mention Hyperparameter tuning}
Each model is trained for 100 epochs to ensure convergence and the best model (based on validation).

\section{Evaluation}
\label{sec:Methods:Evaluation}

\subsection{Metrics}
\label{sec:Methods:Evaluation:Metrics}

The trained models are evaluated using multiple metrics that are described in this section. 

\todo[inline]{explain how models are compared using SRCC, MAE, PLCC, R2}

\todo[inline]{
    Explain metrics MAE, SRCC, PLCC, R2
}

The output of the models developed in this thesis is not directly comparable to other approaches. Therefore, the test data is grouped into focus stacks. Each model is run on all patches of a focus stack. The image with the highest focus score assigned, is considered to be the model's prediction. The prediction is considered to be correct if the image with an in-focus annotation or an image above or below is predicted (see \autoref{fig:Methods:Data:InFocusExampleImages}). The reasoning behind this approach is that during the annotation process it was difficult for annotators to decide on which image is most in focus compared to its neighbours. The same approach is undertaken by \citeauthor{senaras2018deepfocus} (DeepFocus) \cite{senaras2018deepfocus} (see \autoref{sec:Foundations:RelatedWorks:DeepLearning}). To get further understanding of the magnitude of a model's misprediction the \ac{mae} of the index is used.

\begin{figure}[tb]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/+0.0068_+1_stack.jpg}
        \caption{+1}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/+0.0_stack.jpg}
        \caption{in focus}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/30_methods/50_evaluation/-0.0068_-1_stack.jpg}
        \caption{-1}
    \end{subfigure}

    \caption{Figure showing which images are considered to be in focus for the best image selection.}
    \label{fig:Methods:Data:InFocusExampleImages}
\end{figure}


\subsection{Implementation of Traditional Focusing Methods}
\label{sec:Methods:Evaluation:Traditional}

Vollath was implemented using pytorch tensors. This allows GPU computation.

The functions \texttt{sum} and \texttt{mul} represent \texttt{torch.sum} and \texttt{torch.mul}, respectively.

\begin{verbatim*}
sum(mul(IMG(:-1,:), IMG(1:,:))) - sum(mul(IMG(:-2,:), IMG(2:,:)))
\end{verbatim*}

\subsection{Computational Resource Usage}
\label{sec:Methods:Evaluation:Computation}

The speed (performance) of the models is analysed by running each method on 620 image patches. The measurement only includes computation time. Transfer time to the GPU and loading time of images is not considered.

Also, a theoretical analysis is given. Further, the memory consumption is analysed from a theoretical viewpoint.

