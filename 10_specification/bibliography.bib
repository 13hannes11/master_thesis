
@article{adolfsson2021developing,
  title = {Developing a {{Graphical Application}} to {{Control Stepper Motors}} with {{Sensorless Load Detection}}},
  author = {Adolfsson, Mattias},
  year = {2021},
  month = sep,
  pages = {58},
  issn = {1401-5749},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/hannes/Zotero/storage/GI8XKTNZ/Adolfsson - Developing a Graphical Application to Control Step.pdf}
}

@article{berthelot2019mixmatch,
  title = {{{MixMatch}}: {{A Holistic Approach}} to {{Semi-Supervised Learning}}},
  shorttitle = {{{MixMatch}}},
  author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  year = {2019},
  month = oct,
  journal = {arXiv:1905.02249 [cs, stat]},
  eprint = {1905.02249},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/hannes/Zotero/storage/4J47FRIG/Berthelot et al. - 2019 - MixMatch A Holistic Approach to Semi-Supervised L.pdf;/home/hannes/Zotero/storage/NS6KNUXR/1905.html}
}

@article{gidaris2018unsupervised,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.07728 [cs]},
  eprint = {1803.07728},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet .},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/hannes/Zotero/storage/BYU65Q9I/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting.pdf;/home/hannes/Zotero/storage/BAWRKGM6/1803.html}
}

@article{hanna2019whole,
  title = {Whole Slide Imaging Equivalency and Efficiency Study: Experience at a Large Academic Center},
  shorttitle = {Whole Slide Imaging Equivalency and Efficiency Study},
  author = {Hanna, Matthew G. and Reuter, Victor E. and Hameed, Meera R. and Tan, Lee K. and Chiang, Sarah and Sigel, Carlie and Hollmann, Travis and Giri, Dilip and Samboy, Jennifer and Moradel, Carlos and Rosado, Andrea and Otilano, John R. and England, Christine and Corsale, Lorraine and Stamelos, Evangelos and Yagi, Yukako and Sch{\"u}ffler, Peter J. and Fuchs, Thomas and Klimstra, David S. and Sirintrapun, S. Joseph},
  year = {2019},
  month = jul,
  journal = {Modern Pathology},
  volume = {32},
  number = {7},
  pages = {916--928},
  issn = {0893-3952, 1530-0285},
  doi = {10/gg3wtj},
  abstract = {Whole slide imaging is Food and Drug Administration-approved for primary diagnosis in the United States of America; however, relatively few pathology departments in the country have fully implemented an enterprise wide digital pathology system enabled for primary diagnosis. Digital pathology has significant potential to transform pathology practice with several published studies documenting some level of diagnostic equivalence between digital and conventional systems. However, whole slide imaging also has significant potential to disrupt pathology practice, due to the differences in efficiency of manipulating digital images vis-\`a-vis glass slides, and studies on the efficiency of actual digital pathology workload are lacking. Our randomized, equivalency and efficiency study aimed to replicate clinical workflow, comparing conventional microscopy to a complete digital pathology signout using whole slide images, evaluating the equivalency and efficiency of glass slide to whole slide image reporting, reflective of true pathology practice workloads in the clinical setting. All glass slides representing an entire day's routine clinical signout workload for six different anatomic pathology subspecialties at Memorial Sloan Kettering Cancer Center were scanned on Leica Aperio AT2 at \texttimes 40 (0.25 \textmu m/pixel). Integration of whole slide images for each accessioned case is through an interface between the Leica eSlide manager database and the laboratory information system, Cerner CoPathPlus. Pathologists utilized a standard institution computer workstation and viewed whole slide images through an internally developed, vendor agnostic whole slide image viewer, named the ``MSK Slide Viewer''. Subspecialized pathologists first reported on glass slides from surgical pathology cases using routine clinical workflow. Glass slides were de-identified, scanned, and re-accessioned in the laboratory information system test environment. After a washout period of 13 weeks, pathologists reported the same clinical workload using whole slide image integrated within the laboratory information system. Intraobserver equivalency metrics included top-line diagnosis, margin status, lymphovascular and/or perineural invasion, pathology stage, and the need to order ancillary testing (i.e., recuts, immunohistochemistry). Turnaround time (efficiency) evaluation was defined by the start of each case when opened in the laboratory information system and when the case was completed for that day (i.e., case sent to signout queue or pending ancillary studies). Eight pathologists participated from the following subspecialties: bone and soft tissue, genitourinary, gastrointestinal, breast, gynecologic, and dermatopathology. Glass slides signouts comprised of 204 cases, encompassing 2091 glass slides; and digital signouts comprised of 199 cases, encompassing 2073 whole slide images. The median whole slide image file size was 1.54 GB; scan time/slide, 6 min 24 s; and scan area 32.1 \texttimes{} 18.52 mm. Overall diagnostic equivalency (e.g., top-line diagnosis) was 99.3\% between digital and glass slide signout; however, signout using whole slide images showed a median overall 19\% decrease in efficiency per case. No significant difference by reader, subspecialty, or specimen type was identified. Our experience is the most comprehensive study to date and shows high intraobserver whole slide image to glass slide equivalence in reporting of true clinical workflows and workloads. Efficiency needs to improve for digital pathology to gain more traction among pathologists.},
  langid = {english},
  file = {/home/hannes/Zotero/storage/KFI5AQNG/Hanna et al. - 2019 - Whole slide imaging equivalency and efficiency stu.pdf}
}

@article{hosseini2020focus,
  title = {Focus {{Quality Assessment}} of {{High-Throughput Whole Slide Imaging}} in {{Digital Pathology}}},
  author = {Hosseini, Mahdi S. and {Brawley-Hayes}, Jasper A. Z. and Zhang, Yueyang and Chan, Lyndon and Plataniotis, Konstantinos and Damaskinos, Savvas},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Medical Imaging},
  volume = {39},
  number = {1},
  pages = {62--74},
  issn = {0278-0062, 1558-254X},
  doi = {10/gm8ghb},
  abstract = {One of the challenges facing the adoption of digital pathology workflows for clinical use is the need for automated quality control. As the scanners sometimes determine focus inaccurately, the resultant image blur deteriorates the scanned slide to the point of being unusable. Also, the scanned slide images tend to be extremely large when scanned at greater or equal 20X image resolution. Hence, for digital pathology to be clinically useful, it is necessary to use computational tools to quickly and accurately quantify the image focus quality and determine whether an image needs to be re-scanned. We propose a no-reference focus quality assessment metric specifically for digital pathology images that operate by using a sum of even-derivative filter bases to synthesize a human visual system-like kernel, which is modeled as the inverse of the lens' point spread function. This kernel is then applied to a digital pathology image to modify high-frequency image information deteriorated by the scanner's optics and quantify the focus quality at the patch level. We show in several experiments that our method correlates better with ground-truth z-level data than other methods, which is more computationally efficient. We also extend our method to generate a local slide-level focus quality heatmap, which can be used for automated slide quality control, and demonstrate the utility of our method for clinical scan quality control by comparison with subjective slide quality scores.},
  langid = {english},
  file = {/home/hannes/Zotero/storage/6X5KDYCX/Hosseini et al. - 2020 - Focus Quality Assessment of High-Throughput Whole .pdf}
}

@article{kohlberger2019wholeslide,
  title = {Whole-{{Slide Image Focus Quality}}: {{Automatic Assessment}} and {{Impact}} on {{AI Cancer Detection}}},
  shorttitle = {Whole-{{Slide Image Focus Quality}}},
  author = {Kohlberger, Timo and Liu, Yun and Moran, Melissa and {Po-Hsuan} and Chen and Brown, Trissia and Mermel, Craig H. and Hipp, Jason D. and Stumpe, Martin C.},
  year = {2019},
  journal = {Journal of Pathology Informatics},
  volume = {10},
  number = {1},
  eprint = {1901.04619},
  eprinttype = {arxiv},
  pages = {39},
  issn = {2153-3539},
  doi = {10/gm9n9g},
  abstract = {Digital pathology enables remote access or consults and powerful image analysis algorithms. However, the slide digitization process can create artifacts such as out-of-focus (OOF). OOF is often only detected upon careful review, potentially causing rescanning and workflow delays. Although scan-time operator screening for whole-slide OOF is feasible, manual screening for OOF affecting only parts of a slide is impractical. We developed a convolutional neural network (ConvFocus) to exhaustively localize and quantify the severity of OOF regions on digitized slides. ConvFocus was developed using our refined semi-synthetic OOF data generation process, and evaluated using real whole-slide images spanning 3 different tissue types and 3 different stain types that were digitized by two different scanners. ConvFocus's predictions were compared with pathologist-annotated focus quality grades across 514 distinct regions representing 37,700 35x35 \$\textbackslash mu\$m image patches, and 21 digitized "z-stack" whole-slide images that contain known OOF patterns. When compared to pathologist-graded focus quality, ConvFocus achieved Spearman rank coefficients of 0.81 and 0.94 on two scanners, and reproduced the expected OOF patterns from z-stack scanning. We also evaluated the impact of OOF on the accuracy of a state-of-the-art metastatic breast cancer detector and saw a consistent decrease in performance with increasing OOF. Comprehensive whole-slide OOF categorization could enable rescans prior to pathologist review, potentially reducing the impact of digitization focus issues on the clinical workflow. We show that the algorithm trained on our semi-synthetic OOF data generalizes well to real OOF regions across tissue types, stains, and scanners. Finally, quantitative OOF maps can flag regions that might otherwise be misclassified by image analysis algorithms, preventing OOF-induced errors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,foucs detection},
  file = {/home/hannes/Zotero/storage/AY98CIUF/Kohlberger et al. - 2019 - Whole-Slide Image Focus Quality Automatic Assessm.pdf;/home/hannes/Zotero/storage/QEUQUHCG/1901.html}
}

@article{larsson2020development,
  title = {Development of Machine Learning Models for Object Identification of Parasite Eggs Using Microscopy},
  author = {Larsson, Joel and Hedberg, Rasmus},
  year = {2020},
  month = jun,
  issn = {1654-7616},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/hannes/Zotero/storage/TC3N3EUK/Larsson and Hedberg - 2020 - Development of machine learning models for object .pdf;/home/hannes/Zotero/storage/MRSY5MLH/record.html}
}

@article{li2014semisupervised,
  title = {A {{Semi-Supervised Active Learning Framework}} for {{Image Classification}}},
  author = {Li, Han Yi and Yang, Ming and Kang, Nan Nan and Yue, Lu Lu},
  year = {2014},
  month = may,
  journal = {Applied Mechanics and Materials},
  volume = {556--562},
  pages = {4765--4769},
  issn = {1662-7482},
  doi = {10/gnn446},
  abstract = {In this paper, a novel image classification method, incorporating active learning and semi-supervised learning (SSL), is proposed. In this method, two classifiers are needed where one is trained by labeled data and some unlabeled data, while the other one is trained only by labeled data. Specifically, in each round, two classifiers iterate to select useful examples in contention for user query. Then we compute the label changing rate for every unlabeled example in each classifier. Those examples in which the label changing rate is zero and the label in the two classifiers is the same are selected to add into the training data of the first classifier. Our experimental results show that our method significantly reduced the need of labeled examples, while at the same time reducing classification error compared with widely used image classification algorithms.},
  file = {/home/hannes/Zotero/storage/PVG6T33E/Li et al. - 2014 - A Semi-Supervised Active Learning Framework for Im.pdf}
}

@article{senaras2018deepfocus,
  title = {{{DeepFocus}}: {{Detection}} of out-of-Focus Regions in Whole Slide Digital Images Using Deep Learning},
  shorttitle = {{{DeepFocus}}},
  author = {Senaras, Caglar and Niazi, M. Khalid Khan and Lozanski, Gerard and Gurcan, Metin N.},
  editor = {Lo, Chung-Ming},
  year = {2018},
  month = oct,
  journal = {PLOS ONE},
  volume = {13},
  number = {10},
  pages = {e0205387},
  issn = {1932-6203},
  doi = {10/gm9n8p},
  langid = {english},
  keywords = {foucs detection},
  file = {/home/hannes/Zotero/storage/M9VZ57ZF/Senaras et al. - 2018 - DeepFocus Detection of out-of-focus regions in wh.pdf}
}

@misc{theinternationalsocietyforneglectedtropicaldiseases2021artificial,
  title = {Artificial {{Intelligence-Driven Digital Pathology}} to {{Strengthen STH}} \& {{Schistosomiasis Control}}},
  author = {{The International Society for Neglected Tropical Diseases}},
  year = {2021},
  month = jan,
  abstract = {Dr. Lieven Stuyver (Senior Scientific Director, Global Public Health R\&D, Johnson \& Johnson) presents: "Partnering to Leverage Artificial Intelligence-Driven Digital Pathology to Strengthen Soil-Transmitted Helminthiasis and Schistosomiasis Control Programs" As part of the ISNTD Connect panel session: "Access to schistosomiasis innovation: the next frontier to reach elimination by 2030" This session is hosted by ISNTD and includes the following panelists: - Dr. Beatrice Greco (Head of R\&D and Access, Global Health Institute, Merck): "Merck schistosomiasis elimination platform: integrated approach to support elimination" - Dr. Sarah Nogaro (Senior Scientific Officer, NTDs, FIND): "A schistosomiasis rapid diagnostic test to support control programs in monitoring treatment impact and reassessment mapping" - Dr. Lieven Stuyver (Senior Scientific Director, Global Public Health R\&D, Johnson \& Johnson): "Partnering to Leverage Artificial Intelligence-Driven Digital Pathology to Strengthen Soil-Transmitted Helminthiasis and Schistosomiasis Control Programs" - Dr. Beatrice Greco (Head of R\&D and Access, Global Health Institute, Merck): "Access to upcoming schistosomiasis innovations: a path to be created" \hspace{0pt}Recorded on Tuesday 26th January 2021 as part of the online ISNTD Connect series of short meetings, launched during the 2020 COVID19 pandemic to keep researchers and professionals in the fields of global and tropical health connected.  www.isntd.org/isntd-connect}
}

@inproceedings{wang2020focuslitenn,
  title = {{{FocusLiteNN}}: {{High Efficiency Focus Quality Assessment}} for {{Digital Pathology}}},
  shorttitle = {{{FocusLiteNN}}},
  booktitle = {Medical {{Image Computing}} and {{Computer Assisted Intervention}} \textendash{} {{MICCAI}} 2020},
  author = {Wang, Zhongling and Hosseini, Mahdi S. and Miles, Adyn and Plataniotis, Konstantinos N. and Wang, Zhou},
  editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {403--413},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10/gnpd37},
  abstract = {Out-of-focus microscopy lens in digital pathology is a critical bottleneck in high-throughput Whole Slide Image (WSI) scanning platforms, for which pixel-level automated Focus Quality Assessment (FQA) methods are highly desirable to help significantly accelerate the clinical workflows. Existing FQA methods include both knowledge-driven and data-driven approaches. While data-driven approaches such as Convolutional Neural Network (CNN) based methods have shown great promises, they are difficult to use in practice due to their high computational complexity and lack of transferability. Here, we propose a highly efficient CNN-based model that maintains fast computations similar to the knowledge-driven methods without excessive hardware requirements such as GPUs. We create a training dataset using FocusPath which encompasses diverse tissue slides across nine different stain colors, where the stain diversity greatly helps the model to learn diverse color spectrum and tissue structures. In our attempt to reduce the CNN complexity, we find with surprise that even trimming down the CNN to the minimal level, it still achieves a highly competitive performance. We introduce a novel comprehensive evaluation dataset, the largest of its kind, annotated and compiled from TCGA repository for model assessment and comparison, for which the proposed method exhibits superior precision-speed trade-off when compared with existing knowledge-driven and data-driven FQA approaches.},
  isbn = {978-3-030-59722-1},
  langid = {english},
  keywords = {Deep learning,Digital pathology,Focus Quality Assessment,Out-of-focus,Whole Slide Image (WSI)},
  file = {/home/hannes/Zotero/storage/IF3BKJWB/Wang et al. - 2020 - FocusLiteNN High Efficiency Focus Quality Assessm.pdf}
}

@book{worldhealthorganization1991basic,
  title = {Basic Laboratory Methods in Medical Parasitology},
  author = {{World Health Organization}},
  year = {1991},
  publisher = {{World Health Organization}},
  isbn = {92-4-154410-4},
  file = {/home/hannes/Zotero/storage/BLZ7T3NS/9241544104_(part1).pdf;/home/hannes/Zotero/storage/P8GE5X7S/9241544104_(part2).pdf}
}

@misc{worldhealthorganization2020soiltransmitted,
  title = {Soil-Transmitted Helminth Infections},
  author = {{World Health Organization}},
  year = {2020},
  month = mar,
  howpublished = {https://www.who.int/news-room/fact-sheets/detail/soil-transmitted-helminth-infections},
  langid = {english},
  file = {/home/hannes/Zotero/storage/P4BB6D8K/soil-transmitted-helminth-infections.html}
}

@misc{worldhealthorganization2021neglected,
  title = {Neglected Tropical Diseases},
  author = {{World Health Organization}},
  year = {2021},
  month = jan,
  howpublished = {https://www.who.int/news-room/questions-and-answers/item/neglected-tropical-diseases},
  langid = {english},
  file = {/home/hannes/Zotero/storage/EZNTPVZ3/neglected-tropical-diseases.html}
}

@inproceedings{yusun2005autofocusing,
  title = {Autofocusing Algorithm Selection in Computer Microscopy},
  booktitle = {2005 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {{Yu Sun} and Duthaler, S. and Nelson, B.J.},
  year = {2005},
  pages = {70--76},
  publisher = {{IEEE}},
  address = {{Edmonton, Alta., Canada}},
  doi = {10/bh9dkf},
  abstract = {Autofocusing is a fundamental technology for automated biological and biomedical analyses and is indispensable for routine use of microscopes on a large scale. This paper presents a comprehensive comparison study of 18 focus algorithms in which a total of 139,000 microscope images are analyzed. Six samples were used with three observation methods (bright field, phase contrast, and differential interference contrast (DIC)) under two magnifications (100X and 400X). A ranking methodology is proposed, based on which the 18 focus algorithms are ranked. Image pre-processing is also conducted to extensively reveal the performance and robustness of the focus algorithms. The presented guidelines allow for the selection of the optimal focus algorithm for different microscopy applications.},
  isbn = {978-0-7803-8912-0},
  langid = {english},
  file = {/home/hannes/Zotero/storage/NANMWULW/Yu Sun et al. - 2005 - Autofocusing algorithm selection in computer micro.pdf}
}

@article{zhang2021flexmatch,
  title = {{{FlexMatch}}: {{Boosting Semi-Supervised Learning}} with {{Curriculum Pseudo Labeling}}},
  shorttitle = {{{FlexMatch}}},
  author = {Zhang, Bowen and Wang, Yidong and Hou, Wenxin and Wu, Hao and Wang, Jindong and Okumura, Manabu and Shinozaki, Takahiro},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.08263 [cs]},
  eprint = {2110.08263},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The recently proposed FixMatch achieved state-of-the-art results on most semisupervised learning (SSL) benchmarks. However, like other modern SSL algorithms, FixMatch uses a pre-defined constant threshold for all classes to select unlabeled data that contribute to the training, thus failing to consider different learning status and learning difficulties of different classes. To address this issue, we propose Curriculum Pseudo Labeling (CPL), a curriculum learning approach to leverage unlabeled data according to the model's learning status. The core of CPL is to flexibly adjust thresholds for different classes at each time step to let pass informative unlabeled data and their pseudo labels. CPL does not introduce additional parameters or computations (forward or backward propagation). We apply CPL to FixMatch and call our improved algorithm FlexMatch. FlexMatch achieves state-of-the-art performance on a variety of SSL benchmarks, with especially strong performances when the labeled data are extremely limited or when the task is challenging. For example, FlexMatch outperforms FixMatch by 14.32\% and 24.55\% on CIFAR-100 and STL-10 datasets respectively, when there are only 4 labels per class. CPL also significantly boosts the convergence speed, e.g., FlexMatch can use only 1/5 training time of FixMatch to achieve even better performance. Furthermore, we show that CPL can be easily adapted to other SSL algorithms and remarkably improve their performances. We open source our code at https://github.com/TorchSSL/TorchSSL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/hannes/Zotero/storage/AUM8NM5X/Zhang et al. - 2021 - FlexMatch Boosting Semi-Supervised Learning with .pdf}
}


